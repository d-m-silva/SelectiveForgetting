{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3feed7d-13e0-4218-a5b2-abdea8eba131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/lib/oar/.batch_job_bashrc: line 5: /home/dudasilva/.bashrc: No such file or directory\n",
      "fatal: destination path 'SelectiveForgetting' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/d-m-silva/SelectiveForgetting.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95534604-33dd-4cb3-b11d-d967a650e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dudasilva/SelectiveForgetting/SelectiveForgetting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudasilva/unlearning/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd SelectiveForgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0725c51-074c-469a-a680-e610765f8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    " \n",
    "# Returns the number of\n",
    "# objects it has collected\n",
    "# and deallocated\n",
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9927ee-bee6-4109-ac62-543fa2e1f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13957/2905006116.py:45: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import variational\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import itertools\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from models import *\n",
    "\n",
    "import models\n",
    "\n",
    "from logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51137f61-b866-4375-b762-8d3ae7f51fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "\n",
    "    import pdb\n",
    "\n",
    "    pdb.set_trace\n",
    "    \n",
    "def parameter_count(model):\n",
    "\n",
    "    count=0\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "\n",
    "    print(f'Total Number of Parameters: {count}')\n",
    "    \n",
    "def vectorize_params(model):\n",
    "\n",
    "    param = []\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(param)\n",
    "    \n",
    "def print_param_shape(model):\n",
    "\n",
    "    for k,p in model.named_parameters():\n",
    "\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5252a-c2f8-4e59-a69d-658d4549bb8f",
   "metadata": {},
   "source": [
    " ## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c05a82-cb69-4925-9cb6-6fe348b5639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1\n",
      "[Logging in small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_training]\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/dudasilva/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:05<00:00, 29.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/dudasilva/data/cifar-10-python.tar.gz to /home/dudasilva/data\n",
      "Files already downloaded and verified\n",
      "Number of Classes: 10\n",
      "[0] train metrics:{\"loss\": 2.314283561706543, \"error\": 0.855}\n",
      "Learning Rate : 0.1\n",
      "[0] dry_run metrics:{\"loss\": 3.3192060661315916, \"error\": 0.86}\n",
      "Learning Rate : 0.1\n",
      "[0] test metrics:{\"loss\": 2.4529306049346924, \"error\": 0.8619999999850988}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.92 sec\n",
      "[1] train metrics:{\"loss\": 2.7997510623931885, \"error\": 0.88}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.66 sec\n",
      "[2] train metrics:{\"loss\": 3.0096083641052247, \"error\": 0.87}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.92 sec\n",
      "[3] train metrics:{\"loss\": 2.941478967666626, \"error\": 0.8625}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.73 sec\n",
      "[4] train metrics:{\"loss\": 2.389516658782959, \"error\": 0.8425}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.77 sec\n",
      "[5] train metrics:{\"loss\": 2.4274136066436767, \"error\": 0.795}\n",
      "Learning Rate : 0.1\n",
      "[5] dry_run metrics:{\"loss\": 2.2957512187957763, \"error\": 0.76}\n",
      "Learning Rate : 0.1\n",
      "[5] test metrics:{\"loss\": 2.8934261112213133, \"error\": 0.7939999996423721}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.37 sec\n",
      "[6] train metrics:{\"loss\": 2.3093838596343996, \"error\": 0.795}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.77 sec\n",
      "[7] train metrics:{\"loss\": 2.159022665023804, \"error\": 0.78}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.91 sec\n",
      "[8] train metrics:{\"loss\": 2.208447103500366, \"error\": 0.8025}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.88 sec\n",
      "[9] train metrics:{\"loss\": 2.1784866714477538, \"error\": 0.8075}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.9 sec\n",
      "[10] train metrics:{\"loss\": 2.1014205551147462, \"error\": 0.81875}\n",
      "Learning Rate : 0.1\n",
      "[10] dry_run metrics:{\"loss\": 2.1214884233474733, \"error\": 0.7975}\n",
      "Learning Rate : 0.1\n",
      "[10] test metrics:{\"loss\": 2.137377920150757, \"error\": 0.823}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.35 sec\n",
      "[11] train metrics:{\"loss\": 2.1137373065948486, \"error\": 0.7725}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.77 sec\n",
      "[12] train metrics:{\"loss\": 2.030103063583374, \"error\": 0.74875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.86 sec\n",
      "[13] train metrics:{\"loss\": 2.0381999683380125, \"error\": 0.74375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.86 sec\n",
      "[14] train metrics:{\"loss\": 2.0006832885742187, \"error\": 0.75125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.83 sec\n",
      "[15] train metrics:{\"loss\": 1.97056480884552, \"error\": 0.70875}\n",
      "Learning Rate : 0.1\n",
      "[15] dry_run metrics:{\"loss\": 1.9266808128356934, \"error\": 0.68}\n",
      "Learning Rate : 0.1\n",
      "[15] test metrics:{\"loss\": 2.0725102386474608, \"error\": 0.7519999997615814}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.34 sec\n",
      "[16] train metrics:{\"loss\": 1.965289249420166, \"error\": 0.70875}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.91 sec\n",
      "[17] train metrics:{\"loss\": 1.9103712558746337, \"error\": 0.6675}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 2.02 sec\n",
      "[18] train metrics:{\"loss\": 1.9069717025756836, \"error\": 0.70375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.94 sec\n",
      "[19] train metrics:{\"loss\": 1.81251859664917, \"error\": 0.6325}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.87 sec\n",
      "[20] train metrics:{\"loss\": 1.8282555198669435, \"error\": 0.65125}\n",
      "Learning Rate : 0.1\n",
      "[20] dry_run metrics:{\"loss\": 1.7525881242752075, \"error\": 0.64625}\n",
      "Learning Rate : 0.1\n",
      "[20] test metrics:{\"loss\": 1.9279774360656738, \"error\": 0.6909999997615814}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.3 sec\n",
      "[21] train metrics:{\"loss\": 1.8204091262817383, \"error\": 0.65}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.93 sec\n",
      "[22] train metrics:{\"loss\": 1.7400207901000977, \"error\": 0.63375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.87 sec\n",
      "[23] train metrics:{\"loss\": 1.760245819091797, \"error\": 0.635}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.87 sec\n",
      "[24] train metrics:{\"loss\": 1.7526803207397461, \"error\": 0.62125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.86 sec\n",
      "[25] train metrics:{\"loss\": 1.695216908454895, \"error\": 0.6125}\n",
      "Learning Rate : 0.1\n",
      "[25] dry_run metrics:{\"loss\": 1.6606330633163453, \"error\": 0.585}\n",
      "Learning Rate : 0.1\n",
      "[25] test metrics:{\"loss\": 1.9679508790969849, \"error\": 0.6880000002384186}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.27 sec\n",
      "[26] train metrics:{\"loss\": 1.69985671043396, \"error\": 0.61}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.87 sec\n",
      "[27] train metrics:{\"loss\": 1.735387191772461, \"error\": 0.5975}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.78 sec\n",
      "[28] train metrics:{\"loss\": 1.7712664127349853, \"error\": 0.6375}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.96 sec\n",
      "[29] train metrics:{\"loss\": 1.6722162008285522, \"error\": 0.58125}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 1.87 sec\n",
      "[30] train metrics:{\"loss\": 1.6101663208007813, \"error\": 0.56625}\n",
      "Learning Rate : 0.1\n",
      "[30] dry_run metrics:{\"loss\": 1.4650787973403931, \"error\": 0.475}\n",
      "Learning Rate : 0.1\n",
      "[30] test metrics:{\"loss\": 1.8925233039855958, \"error\": 0.6570000011920929}\n",
      "Learning Rate : 0.1\n",
      "Epoch Time: 3.4 sec\n"
     ]
    }
   ],
   "source": [
    "#1) Pretraining on small_cifar10:\n",
    "\n",
    "%run main.py --dataset small_cifar10 --model resnet --filters 0.4 --lr 0.1 --batch-size 128 --lossfn ce --weight-decay 0.0005 --seed 1\n",
    "\n",
    "#2) Pretraining on cifar100:\n",
    "\n",
    "#%run main.py --dataset cifar100 --model resnet --filters 0.4 --lr 0.1 --batch-size 128 --lossfn ce --weight-decay 0.0005 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee252a9-7ef4-437e-9856-463a6ce2e57d",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6a9ab2-f681-4ee1-a653-4f4c0402638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_resnet_0_4_forget_None_lr_0_01_bs_128_ls_ce_wd_0_1_seed_1\n",
      "[Logging in small_cifar5_resnet_0_4_forget_None_lr_0_01_bs_128_ls_ce_wd_0_1_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of Classes: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudasilva/SelectiveForgetting/SelectiveForgetting/main.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(args.resume)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train metrics:{\"loss\": 1.5159034852981568, \"error\": 0.6579999966621399}\n",
      "Learning Rate : 0.01\n",
      "[0] test metrics:{\"loss\": 1.4028399524688722, \"error\": 0.6360000001192093}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.43 sec\n",
      "[1] train metrics:{\"loss\": 1.333157444000244, \"error\": 0.5780000014305114}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.9 sec\n",
      "[2] train metrics:{\"loss\": 1.200620369911194, \"error\": 0.4879999980926514}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[3] train metrics:{\"loss\": 1.1271758279800415, \"error\": 0.45599999856948853}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.92 sec\n",
      "[4] train metrics:{\"loss\": 1.1013252830505371, \"error\": 0.4479999933242798}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.93 sec\n",
      "[5] train metrics:{\"loss\": 1.0609872312545776, \"error\": 0.4379999980926514}\n",
      "Learning Rate : 0.01\n",
      "[5] test metrics:{\"loss\": 1.183127137184143, \"error\": 0.4920000014305115}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.22 sec\n",
      "[6] train metrics:{\"loss\": 1.0458361825942992, \"error\": 0.4400000052452087}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.88 sec\n",
      "[7] train metrics:{\"loss\": 1.0261449222564698, \"error\": 0.41800000047683716}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[8] train metrics:{\"loss\": 1.0053773279190064, \"error\": 0.41599999618530276}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[9] train metrics:{\"loss\": 1.001161804676056, \"error\": 0.39600000381469724}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.92 sec\n",
      "[10] train metrics:{\"loss\": 0.989742826461792, \"error\": 0.4060000057220459}\n",
      "Learning Rate : 0.01\n",
      "[10] test metrics:{\"loss\": 1.1497519655227662, \"error\": 0.4579999997615814}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.2 sec\n",
      "[11] train metrics:{\"loss\": 0.9798211507797241, \"error\": 0.41200000476837156}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[12] train metrics:{\"loss\": 1.0054434938430785, \"error\": 0.3940000033378601}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.9 sec\n",
      "[13] train metrics:{\"loss\": 0.9747347078323364, \"error\": 0.3980000009536743}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[14] train metrics:{\"loss\": 0.9517274618148803, \"error\": 0.3840000028610229}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[15] train metrics:{\"loss\": 0.9655003070831298, \"error\": 0.3780000009536743}\n",
      "Learning Rate : 0.01\n",
      "[15] test metrics:{\"loss\": 1.1543196640014648, \"error\": 0.4580000021457672}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.21 sec\n",
      "[16] train metrics:{\"loss\": 0.9485712766647338, \"error\": 0.3700000033378601}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[17] train metrics:{\"loss\": 0.9446692476272583, \"error\": 0.36400000190734866}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.88 sec\n",
      "[18] train metrics:{\"loss\": 0.9469134373664856, \"error\": 0.3699999966621399}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[19] train metrics:{\"loss\": 0.9327234044075012, \"error\": 0.34800000095367434}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[20] train metrics:{\"loss\": 0.9283406672477722, \"error\": 0.3519999990463257}\n",
      "Learning Rate : 0.01\n",
      "[20] test metrics:{\"loss\": 1.1569578037261963, \"error\": 0.4379999980926514}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.19 sec\n",
      "[21] train metrics:{\"loss\": 0.9247106666564942, \"error\": 0.35200000572204587}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.88 sec\n",
      "[22] train metrics:{\"loss\": 0.9287982149124145, \"error\": 0.3720000014305115}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.9 sec\n",
      "[23] train metrics:{\"loss\": 0.9202672080993652, \"error\": 0.3640000057220459}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[24] train metrics:{\"loss\": 0.913866000175476, \"error\": 0.3499999942779541}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.88 sec\n",
      "[25] train metrics:{\"loss\": 0.9166480913162232, \"error\": 0.34799999856948854}\n",
      "Learning Rate : 0.01\n",
      "[25] test metrics:{\"loss\": 1.1628626232147217, \"error\": 0.4299999957084656}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.17 sec\n",
      "[26] train metrics:{\"loss\": 0.9252379245758057, \"error\": 0.34999999475479127}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.86 sec\n",
      "[27] train metrics:{\"loss\": 0.9209656114578247, \"error\": 0.3459999966621399}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.79 sec\n",
      "[28] train metrics:{\"loss\": 0.922106430053711, \"error\": 0.3460000057220459}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.9 sec\n",
      "[29] train metrics:{\"loss\": 0.9147660431861877, \"error\": 0.34799999475479126}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.87 sec\n",
      "[30] train metrics:{\"loss\": 0.9058558735847473, \"error\": 0.3499999966621399}\n",
      "Learning Rate : 0.01\n",
      "[30] test metrics:{\"loss\": 1.1708912658691406, \"error\": 0.4359999938011169}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.17 sec\n"
     ]
    }
   ],
   "source": [
    "#TRAINING ON small_cifar_5 !!! WITHOUT FORGETTING !!!\n",
    "\n",
    "#1) Using pretained small_cifar10 as checkpoint -> lighter\n",
    "\n",
    "%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --seed 1\n",
    "\n",
    "#2) Using pretained CIFAR100 as checkpoint -> heavier\n",
    "\n",
    "#%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e87a71-4e68-494f-ac98-e69836281c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint name: small_cifar5_resnet_0_4_forget_0_num_25_lr_0_01_bs_128_ls_ce_wd_0_1_seed_1\n",
      "[Logging in small_cifar5_resnet_0_4_forget_0_num_25_lr_0_01_bs_128_ls_ce_wd_0_1_seed_1_training]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Replacing indexes [26 86  2 55 75 93 16 73 54 95 53 92 78 13  7 30 22 24 33  8 43 62  3 71\n",
      " 45]\n",
      "Number of Classes: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudasilva/SelectiveForgetting/SelectiveForgetting/main.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(args.resume)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train metrics:{\"loss\": 1.503102180480957, \"error\": 0.6419999990463257}\n",
      "Learning Rate : 0.01\n",
      "[0] test metrics:{\"loss\": 1.4044535818099975, \"error\": 0.6320000003576278}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.28 sec\n",
      "[1] train metrics:{\"loss\": 1.3137593870162965, \"error\": 0.56400000166893}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.0 sec\n",
      "[2] train metrics:{\"loss\": 1.1830432481765747, \"error\": 0.4959999957084656}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[3] train metrics:{\"loss\": 1.1180360469818116, \"error\": 0.46200000095367433}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.83 sec\n",
      "[4] train metrics:{\"loss\": 1.0832451620101928, \"error\": 0.4479999933242798}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.75 sec\n",
      "[5] train metrics:{\"loss\": 1.0386791048049926, \"error\": 0.41999999380111697}\n",
      "Learning Rate : 0.01\n",
      "[5] test metrics:{\"loss\": 1.1925303316116334, \"error\": 0.5020000030994415}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.09 sec\n",
      "[6] train metrics:{\"loss\": 1.0323348574638367, \"error\": 0.4379999938011169}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[7] train metrics:{\"loss\": 1.0117221012115478, \"error\": 0.41800000047683716}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.76 sec\n",
      "[8] train metrics:{\"loss\": 0.9916511783599854, \"error\": 0.41400000524520875}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[9] train metrics:{\"loss\": 0.9888554677963257, \"error\": 0.40000000381469725}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[10] train metrics:{\"loss\": 0.9762329216003418, \"error\": 0.3799999942779541}\n",
      "Learning Rate : 0.01\n",
      "[10] test metrics:{\"loss\": 1.1591416225433349, \"error\": 0.46799999713897705}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.22 sec\n",
      "[11] train metrics:{\"loss\": 0.9635963478088378, \"error\": 0.38999999618530273}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.88 sec\n",
      "[12] train metrics:{\"loss\": 0.9939004411697387, \"error\": 0.39599999380111695}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.93 sec\n",
      "[13] train metrics:{\"loss\": 0.9753687057495117, \"error\": 0.39799999856948853}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.9 sec\n",
      "[14] train metrics:{\"loss\": 0.943314682006836, \"error\": 0.38399999380111693}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[15] train metrics:{\"loss\": 0.9494209384918213, \"error\": 0.3739999942779541}\n",
      "Learning Rate : 0.01\n",
      "[15] test metrics:{\"loss\": 1.1611356401443482, \"error\": 0.4659999969005585}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.1 sec\n",
      "[16] train metrics:{\"loss\": 0.9412784719467163, \"error\": 0.3679999942779541}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.75 sec\n",
      "[17] train metrics:{\"loss\": 0.9383863453865051, \"error\": 0.36199999713897707}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.77 sec\n",
      "[18] train metrics:{\"loss\": 0.9485948014259339, \"error\": 0.3820000033378601}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.73 sec\n",
      "[19] train metrics:{\"loss\": 0.924043221950531, \"error\": 0.34600000095367434}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.74 sec\n",
      "[20] train metrics:{\"loss\": 0.9254018802642823, \"error\": 0.3539999966621399}\n",
      "Learning Rate : 0.01\n",
      "[20] test metrics:{\"loss\": 1.160966856956482, \"error\": 0.4559999976158142}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.0 sec\n",
      "[21] train metrics:{\"loss\": 0.9155394606590271, \"error\": 0.3499999942779541}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.73 sec\n",
      "[22] train metrics:{\"loss\": 0.9131906423568725, \"error\": 0.3620000014305115}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.75 sec\n",
      "[23] train metrics:{\"loss\": 0.9069780797958374, \"error\": 0.34800000333786013}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.75 sec\n",
      "[24] train metrics:{\"loss\": 0.9021377210617065, \"error\": 0.33599999666213987}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.75 sec\n",
      "[25] train metrics:{\"loss\": 0.9037743911743165, \"error\": 0.3240000057220459}\n",
      "Learning Rate : 0.01\n",
      "[25] test metrics:{\"loss\": 1.1614722728729248, \"error\": 0.44}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.23 sec\n",
      "[26] train metrics:{\"loss\": 0.9061797647476196, \"error\": 0.34799999713897706}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.91 sec\n",
      "[27] train metrics:{\"loss\": 0.9030899505615234, \"error\": 0.33199999475479125}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.93 sec\n",
      "[28] train metrics:{\"loss\": 0.8966352753639221, \"error\": 0.35000000333786013}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.89 sec\n",
      "[29] train metrics:{\"loss\": 0.8976573033332824, \"error\": 0.33800000190734864}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 0.92 sec\n",
      "[30] train metrics:{\"loss\": 0.8951117296218872, \"error\": 0.33200000143051145}\n",
      "Learning Rate : 0.01\n",
      "[30] test metrics:{\"loss\": 1.1756478309631349, \"error\": 0.45200000047683714}\n",
      "Learning Rate : 0.01\n",
      "Epoch Time: 1.2 sec\n"
     ]
    }
   ],
   "source": [
    "#TRAINING ON small_cifar_5 !!! WITH FORGETTING !!!\n",
    "\n",
    "#1) Using pretained small_cifar10 as checkpoint -> lighter\n",
    "\n",
    "%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --forget-class 0 --num-to-forget 25 --seed 1\n",
    "\n",
    "#2) Using pretained CIFAR100 as checkpoint -> heavier\n",
    "\n",
    "#%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --forget-class 0 --num-to-forget 25 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c9b52-b677-4c5f-9728-f64ce2ef976f",
   "metadata": {},
   "source": [
    "## Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b5e475-03ff-45a3-8e0a-d6774a61cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 1787741\n"
     ]
    }
   ],
   "source": [
    "log_dict={}\n",
    "\n",
    "training_epochs=25\n",
    "\n",
    "log_dict['epoch']=training_epochs\n",
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8ffdc-9f9c-4e10-8882-616346797d75",
   "metadata": {},
   "source": [
    "## Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5645e3ea-530e-424a-a2ad-e0d107a9abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13957/286994155.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(m_name))\n",
      "/tmp/ipykernel_13957/286994155.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model0.load_state_dict(torch.load(m0_name))\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "\n",
    "arch = args.model\n",
    "\n",
    "filters=args.filters\n",
    "\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "\n",
    "augment = False\n",
    "\n",
    "dataset = args.dataset\n",
    "\n",
    "class_to_forget = args.forget_class\n",
    "\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "\n",
    "num_classes=args.num_classes\n",
    "\n",
    "num_to_forget = args.num_to_forget\n",
    "\n",
    "num_total = len(train_loader.dataset)\n",
    "\n",
    "num_to_retain = num_total - num_to_forget\n",
    "\n",
    "seed = args.seed\n",
    "\n",
    "unfreeze_start = None\n",
    "\n",
    "\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "\n",
    "#model.cuda()\n",
    "\n",
    "#model0.cuda()\n",
    "\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "for p in model0.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea31ac1c-042e-4a90-b6e2-92d2ce3f66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969f45b-087c-4478-8b28-391648f55a76",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6797453-07ab-4016-a24d-d994025bd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "\n",
    "    distance=0\n",
    "\n",
    "    normalization=0\n",
    "\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "\n",
    "        space='  ' if 'bias' in k else ''\n",
    "\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "\n",
    "        distance+=current_dist\n",
    "\n",
    "        normalization+=current_norm\n",
    "\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3a287b3-dfa1-4631-b047-5ffa783630e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.2779924594026405\n",
      "Normalized Distance: 0.005568791383362767\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee0f4c-80c5-41f5-8701-cb63da00f0b5",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f7acefc-4113-4cf0-9a5d-a6ad8d4eefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "\n",
    "    manual_seed(seed)\n",
    "\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c2f86d5-74e6-4d4d-8bae-5cdd5110c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13957/3555119760.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_init.load_state_dict(torch.load(resume))\n"
     ]
    }
   ],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "for p in model_init.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40bf6a05-ac06-41a3-9a41-ee819e24f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 1.3335334427337733\n",
      "Normalized Distance: 0.02675236830634745\n"
     ]
    }
   ],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4de73-ece6-433f-9fae-3b2fb875fcd6",
   "metadata": {},
   "source": [
    "**Data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23325850-5a30-4eb4-9168-1120cddbc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Replacing indexes [26 86  2 55 75 93 16 73 54 95 53 92 78 13  7 30 22 24 33  8 43 62  3 71\n",
      " 45]\n"
     ]
    }
   ],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, augment=False, shuffle=True)\n",
    "\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, augment=False, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "\n",
    "    manual_seed(seed)\n",
    "\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "\n",
    "    def _init_fn(worker_id):\n",
    "\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "\n",
    "\n",
    "\n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "\n",
    "marked = forget_dataset.targets < 0\n",
    "\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "\n",
    "marked = retain_dataset.targets >= 0\n",
    "\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb505f6-5f6f-4ee8-b9ae-ea5732afb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the checkpoints to clear some memory except init_checkpoint (used after!!!!)\n",
    "\n",
    "checkpoints_folder = os.path.dirname(init_checkpoint)\n",
    "\n",
    "files = os.listdir(checkpoints_folder)\n",
    "\n",
    "for file in files:\n",
    "    full_path = os.path.join(checkpoints_folder, file)\n",
    "    if full_path != init_checkpoint:\n",
    "        os.remove(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe797f-3bac-4d9a-9bb6-535aa64a406f",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae39b8-b4ef-4059-8b00-79bf4b1f91e3",
   "metadata": {},
   "source": [
    "**NTK update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75717883-9272-4501-afb5-6ebe0215d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "\n",
    "    model_init.eval()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    G_list = []\n",
    "\n",
    "    f0_minus_y = []\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "\n",
    "        input, target = batch\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            input = input.view(input.shape[0],-1)\n",
    "\n",
    "        target = target.cpu().detach().numpy()\n",
    "\n",
    "        output = model_init(input)\n",
    "\n",
    "        G_sample=[]\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "\n",
    "            G_sample.append(grads)\n",
    "\n",
    "            G_list.append(grads)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "\n",
    "            #loss_hess = np.eye(len(p))\n",
    "\n",
    "            target = 2*target-1\n",
    "\n",
    "            f0_y_update = p-target\n",
    "\n",
    "        elif args.lossfn=='ce':\n",
    "\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "\n",
    "            p[target]-=1\n",
    "\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed88e93-ef46-4671-952c-03e746617924",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ec36f67-4d60-4b1d-b605-0786af78990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13957/3555119760.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_init.load_state_dict(torch.load(resume))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete') \n",
    "\n",
    "#Code expects a directory to store the NTK results, \"NTK_data\"\n",
    "\n",
    "output_dir = \"NTK_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Saves results in the created directory\n",
    "\n",
    "np.save(os.path.join(output_dir, 'G_r.npy'), G_r)\n",
    "np.save(os.path.join(output_dir, 'f0_minus_y_r.npy'), f0_minus_y_r)\n",
    "\n",
    "\n",
    "#np.save('NTK_data/G_r.npy',G_r)\n",
    "\n",
    "#np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "\n",
    "\n",
    "#model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "\n",
    "del G_f, f0_minus_y_f\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e44d37f-de38-428b-98b9-dd3d58718e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "\n",
    "del G, G_f, G_r\n",
    "\n",
    "\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a2a8d-ce88-4b7f-823c-16a597ba0bfc",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc053df8-44d9-4c63-839c-2028e91f0a38",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d9ffda9-f0a6-4708-8d17-8c41ce6a1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "del G\n",
    "\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "del G, f0_minus_y, theta_inv, w_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbbbeb-eaf5-4551-8b3a-91eb271612cd",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5caae08d-dcca-4f1d-9b28-90a5c133b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "del G_r\n",
    "\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e5e09-e615-4ec0-a9dc-2bb733b7c725",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "079665ac-5a7d-4088-821c-269112c8e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feba7c19-490b-4da9-944a-2b70dd4e096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb9668-06be-4563-8cfe-19e7452ed1da",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d1ee31e-6607-4b8b-b9a7-cc81113fe558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Norm-: 0.2779897153377533\n",
      "Predtn Norm-: 0.13829437972073208\n",
      "Actual Scale: 2.0101302446210627\n"
     ]
    }
   ],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ad80b-d487-4fda-bb4e-6cc71501df67",
   "metadata": {},
   "source": [
    "#### Trapezium trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d814c3b-071d-43c7-bb4d-d864d7fe3fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta w -------: 0.13829437972073208\n",
      "Inner Product--: -0.11583815160652983\n",
      "Angle----------:  0.11609879044957316\n",
      "Pred Act Norm--:  0.4260177139710507\n",
      "Predicted Scale:  3.080513574241696\n"
     ]
    }
   ],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner)\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "predicted_scale\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b0a6e-d139-40be-b2c1-44aba6d4c6e0",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36a286d2-be2f-4087-b71e-430c7f296cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "\n",
    "    print(nip)\n",
    "\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "705a36d3-c72a-4fe8-a42e-85a352ea7fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37400860388768165\n"
     ]
    }
   ],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844c4c5-9776-4836-88c6-4ed54dd9b2e8",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "793fe3d3-3115-4aae-9aed-1a117ab29e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "\n",
    "    # Give normalized delta_w\n",
    "\n",
    "    delta_w_dict = OrderedDict()\n",
    "\n",
    "    params_visited = 0\n",
    "\n",
    "    for k,p in model.named_parameters():\n",
    "\n",
    "        num_params = np.prod(list(p.shape))\n",
    "\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "\n",
    "        params_visited+=num_params\n",
    "\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35468d64-76ff-4465-8bce-38cb37db889b",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a92796bd-1d0a-454c-a1ec-4b176a6552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    activations=[]\n",
    "\n",
    "    predictions=[]\n",
    "\n",
    "    if use_bn:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            target=(2*target-1)\n",
    "\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            data=data.view(data.shape[0],-1)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "\n",
    "        if samples_correctness:\n",
    "\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "\n",
    "            predictions.append(get_error(output,target))\n",
    "\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "\n",
    "    if samples_correctness:\n",
    "\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37ab7d27-7cc6-46b9-ae02-e40450a727c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fbe9c35-6073-426a-b2da-c490763f3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12d1bd55-208a-412c-8bc4-1f131a1de8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c41de7-c610-421c-91bb-3425449741ef",
   "metadata": {},
   "source": [
    "### Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ada0e6e3-99a8-405f-b484-2cbf06c5b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Model_D_r -> Loss:0.823, Error:0.33473684210526317\n",
      "Original_Model_D_f -> Loss:0.626, Error:0.12\n",
      "Original_Model_D_t -> Loss:1.074, Error:0.43\n"
     ]
    }
   ],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(model,retain_loader,'Original_Model_D_r')\n",
    "\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(model,forget_loader,'Original_Model_D_f')\n",
    "\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(model,test_loader_full,'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e93eb-d564-4bb5-86a6-a765c92cf37f",
   "metadata": {},
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abc14c90-3017-4157-a572-a42b3f960925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain_Model_D_r -> Loss:0.814, Error:0.32842105263157895\n",
      "Retrain_Model_D_f -> Loss:0.993, Error:0.44\n",
      "Retrain_Model_D_t -> Loss:1.075, Error:0.44\n"
     ]
    }
   ],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(model0,retain_loader,'Retrain_Model_D_r')\n",
    "\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(model0,forget_loader,'Retrain_Model_D_f')\n",
    "\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(model0,test_loader_full,'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb8485-3037-4a1c-968b-2801c041612b",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8df09f78-514d-450c-9ce4-633bb6e7584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "\n",
    "for k,p in model_scrub.named_parameters():\n",
    "\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbb7268f-2d37-4513-8cc4-0a993f2e5d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTK_D_r -> Loss:0.815, Error:0.33052631578947367\n",
      "NTK_D_f -> Loss:0.971, Error:0.44\n",
      "NTK_D_t -> Loss:1.079, Error:0.444\n"
     ]
    }
   ],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(model_scrub,retain_loader,'NTK_D_r')\n",
    "\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(model_scrub,forget_loader,'NTK_D_f')\n",
    "\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(model_scrub,test_loader_full,'NTK_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "acdb4d1c-65e2-463a-aa13-118a8009e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Distance Retrain_Original_D_f -> 8.0\n",
      "Predictions Distance Retrain_NTK_D_f -> 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions_distance(m0_D_f_predictions,m_D_f_predictions,'Retrain_Original_D_f')\n",
    "\n",
    "predictions_distance(m0_D_f_predictions,ntk_D_f_predictions,'Retrain_NTK_D_f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7a0b-a245-425c-b65b-e487dbbaa501",
   "metadata": {},
   "source": [
    "### Activations Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64aa865c-bd33-43db-8073-26a1a8e6466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations Distance Retrain_Original_D_f -> 0.28995272517204285\n",
      "Activations Distance Retrain_Original_D_r -> 0.11064248532056808\n",
      "Activations Distance Retrain_Original_D_t -> 0.11899513006210327\n",
      "Activations Distance Retrain_NTK_D_f -> 0.09318870306015015\n",
      "Activations Distance Retrain_NTK_D_r -> 0.0781976580619812\n",
      "Activations Distance Retrain_NTK_D_t -> 0.08619366586208344\n"
     ]
    }
   ],
   "source": [
    "activations_distance(m0_D_f_activations,m_D_f_activations,'Retrain_Original_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,m_D_r_activations,'Retrain_Original_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,m_D_t_activations,'Retrain_Original_D_t')\n",
    "\n",
    "activations_distance(m0_D_f_activations,ntk_D_f_activations,'Retrain_NTK_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,ntk_D_r_activations,'Retrain_NTK_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,ntk_D_t_activations,'Retrain_NTK_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ddebce-af31-4b10-a832-dfba93d3b0f4",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217c0cd-6e1c-4720-bb43-4823e93aa97a",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8f09710-8f96-4e04-9e4a-d0121b3ca59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    activations=[]\n",
    "\n",
    "    predictions=[]\n",
    "\n",
    "    if use_bn:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            target=(2*target-1)\n",
    "\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            data=data.view(data.shape[0],-1)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        if scrub_act:\n",
    "\n",
    "            G = []\n",
    "\n",
    "            for cls in range(num_classes):\n",
    "\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "                G.append(grads)\n",
    "\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "\n",
    "            G = torch.stack(G).pow(2)\n",
    "\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "\n",
    "        if samples_correctness:\n",
    "\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "\n",
    "            predictions.append(get_error(output,target))\n",
    "\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "\n",
    "    if samples_correctness:\n",
    "\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return metrics.avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python unlearning",
   "language": "python",
   "name": "unlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
