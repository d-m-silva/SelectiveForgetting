{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b205cac-88a4-4407-88d7-02f482f08256",
   "metadata": {},
   "source": [
    "Uncomment cell below if repository wasn't cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3feed7d-13e0-4218-a5b2-abdea8eba131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/d-m-silva/SelectiveForgetting.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95534604-33dd-4cb3-b11d-d967a650e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd SelectiveForgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0725c51-074c-469a-a680-e610765f8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    " \n",
    "# Returns the number of objects it has collected and deallocated\n",
    "\n",
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9927ee-bee6-4109-ac62-543fa2e1f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import variational\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import itertools\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from models import *\n",
    "\n",
    "import models\n",
    "\n",
    "from logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51137f61-b866-4375-b762-8d3ae7f51fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb():\n",
    "\n",
    "    import pdb\n",
    "\n",
    "    pdb.set_trace\n",
    "    \n",
    "def parameter_count(model):\n",
    "\n",
    "    count=0\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "\n",
    "    print(f'Total Number of Parameters: {count}')\n",
    "    \n",
    "def vectorize_params(model):\n",
    "\n",
    "    param = []\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(param)\n",
    "    \n",
    "def print_param_shape(model):\n",
    "\n",
    "    for k,p in model.named_parameters():\n",
    "\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5252a-c2f8-4e59-a69d-658d4549bb8f",
   "metadata": {},
   "source": [
    " ## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c05a82-cb69-4925-9cb6-6fe348b5639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Pretraining on small_cifar10:\n",
    "\n",
    "%run main.py --dataset small_cifar10 --model resnet --filters 0.4 --lr 0.1 --batch-size 128 --lossfn ce --weight-decay 0.0005 --seed 1\n",
    "\n",
    "#2) Pretraining on cifar100:\n",
    "\n",
    "#%run main.py --dataset cifar100 --model resnet --filters 0.4 --lr 0.1 --batch-size 128 --lossfn ce --weight-decay 0.0005 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee252a9-7ef4-437e-9856-463a6ce2e57d",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a9ab2-f681-4ee1-a653-4f4c0402638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING ON small_cifar_5 !!! WITHOUT FORGETTING !!!\n",
    "\n",
    "#1) Using pretained small_cifar10 as checkpoint -> lighter\n",
    "\n",
    "%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --seed 1\n",
    "\n",
    "#2) Using pretained CIFAR100 as checkpoint -> heavier\n",
    "\n",
    "#%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e87a71-4e68-494f-ac98-e69836281c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING ON small_cifar_5 !!! WITH FORGETTING !!!\n",
    "\n",
    "#1) Using pretained small_cifar10 as checkpoint -> lighter\n",
    "\n",
    "%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --forget-class 0 --num-to-forget 25 --seed 1\n",
    "\n",
    "#2) Using pretained CIFAR100 as checkpoint -> heavier\n",
    "\n",
    "#%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --forget-class 0 --num-to-forget 25 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c9b52-b677-4c5f-9728-f64ce2ef976f",
   "metadata": {},
   "source": [
    "## Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b5e475-03ff-45a3-8e0a-d6774a61cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "\n",
    "training_epochs=25\n",
    "\n",
    "log_dict['epoch']=training_epochs\n",
    "\n",
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8ffdc-9f9c-4e10-8882-616346797d75",
   "metadata": {},
   "source": [
    "## Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645e3ea-530e-424a-a2ad-e0d107a9abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "arch = args.model\n",
    "\n",
    "filters=args.filters\n",
    "\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "\n",
    "augment = False\n",
    "\n",
    "dataset = args.dataset\n",
    "\n",
    "class_to_forget = args.forget_class\n",
    "\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "\n",
    "num_classes=args.num_classes\n",
    "\n",
    "num_to_forget = args.num_to_forget\n",
    "\n",
    "num_total = len(train_loader.dataset)\n",
    "\n",
    "num_to_retain = num_total - num_to_forget\n",
    "\n",
    "seed = args.seed\n",
    "\n",
    "unfreeze_start = None\n",
    "\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    model0.cuda()\n",
    "\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "for p in model0.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31ac1c-042e-4a90-b6e2-92d2ce3f66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969f45b-087c-4478-8b28-391648f55a76",
   "metadata": {},
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6797453-07ab-4016-a24d-d994025bd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "\n",
    "    distance=0\n",
    "\n",
    "    normalization=0\n",
    "\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "\n",
    "        space='  ' if 'bias' in k else ''\n",
    "\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "\n",
    "        distance+=current_dist\n",
    "\n",
    "        normalization+=current_norm\n",
    "\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a287b3-dfa1-4631-b047-5ffa783630e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee0f4c-80c5-41f5-8701-cb63da00f0b5",
   "metadata": {},
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7acefc-4113-4cf0-9a5d-a6ad8d4eefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "\n",
    "    manual_seed(seed)\n",
    "\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f86d5-74e6-4d4d-8bae-5cdd5110c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "for p in model_init.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf6a05-ac06-41a3-9a41-ee819e24f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4de73-ece6-433f-9fae-3b2fb875fcd6",
   "metadata": {},
   "source": [
    "**Data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23325850-5a30-4eb4-9168-1120cddbc977",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, augment=False, shuffle=True)\n",
    "\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, augment=False, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "\n",
    "    manual_seed(seed)\n",
    "\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "\n",
    "    def _init_fn(worker_id):\n",
    "\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "\n",
    "\n",
    "\n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "\n",
    "marked = forget_dataset.targets < 0\n",
    "\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "\n",
    "marked = retain_dataset.targets >= 0\n",
    "\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08578191-8bd0-4646-abe8-129a98b5c6fd",
   "metadata": {},
   "source": [
    "### Remove the checkpoints to clear some memory except init_checkpoint (used after!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb505f6-5f6f-4ee8-b9ae-ea5732afb139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoints_folder = os.path.dirname(init_checkpoint)\n",
    "\n",
    "files = os.listdir(checkpoints_folder)\n",
    "\n",
    "for file in files:\n",
    "    full_path = os.path.join(checkpoints_folder, file)\n",
    "    if full_path != init_checkpoint:\n",
    "        os.remove(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe797f-3bac-4d9a-9bb6-535aa64a406f",
   "metadata": {},
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae39b8-b4ef-4059-8b00-79bf4b1f91e3",
   "metadata": {},
   "source": [
    "**NTK update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75717883-9272-4501-afb5-6ebe0215d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "\n",
    "    model_init.eval()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    G_list = []\n",
    "\n",
    "    f0_minus_y = []\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "\n",
    "        input, target = batch\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            input = input.view(input.shape[0],-1)\n",
    "\n",
    "        target = target.cpu().detach().numpy()\n",
    "\n",
    "        output = model_init(input)\n",
    "\n",
    "        G_sample=[]\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "\n",
    "            G_sample.append(grads)\n",
    "\n",
    "            G_list.append(grads)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "\n",
    "            #loss_hess = np.eye(len(p))\n",
    "\n",
    "            target = 2*target-1\n",
    "\n",
    "            f0_y_update = p-target\n",
    "\n",
    "        elif args.lossfn=='ce':\n",
    "\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "\n",
    "            p[target]-=1\n",
    "\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed88e93-ef46-4671-952c-03e746617924",
   "metadata": {},
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec36f67-4d60-4b1d-b605-0786af78990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete') \n",
    "\n",
    "#Code expects a directory to store the NTK results, \"NTK_data\"\n",
    "\n",
    "output_dir = \"NTK_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Saves results in the created directory\n",
    "\n",
    "np.save(os.path.join(output_dir, 'G_r.npy'), G_r)\n",
    "np.save(os.path.join(output_dir, 'f0_minus_y_r.npy'), f0_minus_y_r)\n",
    "\n",
    "\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "\n",
    "del G_f, f0_minus_y_f\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44d37f-de38-428b-98b9-dd3d58718e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "\n",
    "del G, G_f, G_r\n",
    "\n",
    "\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a2a8d-ce88-4b7f-823c-16a597ba0bfc",
   "metadata": {},
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc053df8-44d9-4c63-839c-2028e91f0a38",
   "metadata": {},
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ffda9-f0a6-4708-8d17-8c41ce6a1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "del G\n",
    "\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "del G, f0_minus_y, theta_inv, w_complete\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbbbeb-eaf5-4551-8b3a-91eb271612cd",
   "metadata": {},
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caae08d-dcca-4f1d-9b28-90a5c133b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "del G_r\n",
    "\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e5e09-e615-4ec0-a9dc-2bb733b7c725",
   "metadata": {},
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079665ac-5a7d-4088-821c-269112c8e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb9668-06be-4563-8cfe-19e7452ed1da",
   "metadata": {},
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ee31e-6607-4b8b-b9a7-cc81113fe558",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ad80b-d487-4fda-bb4e-6cc71501df67",
   "metadata": {},
   "source": [
    "#### Trapezium trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d814c3b-071d-43c7-bb4d-d864d7fe3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "if inner<0:\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "else:\n",
    "    angle = np.arccos(inner)\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "predicted_scale = predicted_norm/np.linalg.norm(delta_w)\n",
    "\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b0a6e-d139-40be-b2c1-44aba6d4c6e0",
   "metadata": {},
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a286d2-be2f-4087-b71e-430c7f296cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "\n",
    "    print(nip)\n",
    "\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a36d3-c72a-4fe8-a42e-85a352ea7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "\n",
    "log_dict['nip']=nip\n",
    "\n",
    "del delta_w_actual \n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844c4c5-9776-4836-88c6-4ed54dd9b2e8",
   "metadata": {},
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793fe3d3-3115-4aae-9aed-1a117ab29e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "\n",
    "    # Give normalized delta_w\n",
    "\n",
    "    delta_w_dict = OrderedDict()\n",
    "\n",
    "    params_visited = 0\n",
    "\n",
    "    for k,p in model.named_parameters():\n",
    "\n",
    "        num_params = np.prod(list(p.shape))\n",
    "\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "\n",
    "        params_visited+=num_params\n",
    "\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35468d64-76ff-4465-8bce-38cb37db889b",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92796bd-1d0a-454c-a1ec-4b176a6552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    activations=[]\n",
    "\n",
    "    predictions=[]\n",
    "\n",
    "    if use_bn:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            target=(2*target-1)\n",
    "\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            data=data.view(data.shape[0],-1)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "\n",
    "        if samples_correctness:\n",
    "\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "\n",
    "            predictions.append(get_error(output,target))\n",
    "\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "\n",
    "    if samples_correctness:\n",
    "\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab7d27-7cc6-46b9-ae02-e40450a727c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe9c35-6073-426a-b2da-c490763f3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1bd55-208a-412c-8bc4-1f131a1de8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c41de7-c610-421c-91bb-3425449741ef",
   "metadata": {},
   "source": [
    "### Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0e6e3-99a8-405f-b484-2cbf06c5b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(model,retain_loader,'Original_Model_D_r')\n",
    "\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(model,forget_loader,'Original_Model_D_f')\n",
    "\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(model,test_loader_full,'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e93eb-d564-4bb5-86a6-a765c92cf37f",
   "metadata": {},
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc14c90-3017-4157-a572-a42b3f960925",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(model0,retain_loader,'Retrain_Model_D_r')\n",
    "\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(model0,forget_loader,'Retrain_Model_D_f')\n",
    "\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(model0,test_loader_full,'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb8485-3037-4a1c-968b-2801c041612b",
   "metadata": {},
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df09f78-514d-450c-9ce4-633bb6e7584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "\n",
    "for k,p in model_scrub.named_parameters():\n",
    "\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7268f-2d37-4513-8cc4-0a993f2e5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(model_scrub,retain_loader,'NTK_D_r')\n",
    "\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(model_scrub,forget_loader,'NTK_D_f')\n",
    "\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(model_scrub,test_loader_full,'NTK_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb4d1c-65e2-463a-aa13-118a8009e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,m_D_f_predictions,'Retrain_Original_D_f')\n",
    "\n",
    "predictions_distance(m0_D_f_predictions,ntk_D_f_predictions,'Retrain_NTK_D_f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f7a0b-a245-425c-b65b-e487dbbaa501",
   "metadata": {},
   "source": [
    "### Activations Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa865c-bd33-43db-8073-26a1a8e6466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_distance(m0_D_f_activations,m_D_f_activations,'Retrain_Original_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,m_D_r_activations,'Retrain_Original_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,m_D_t_activations,'Retrain_Original_D_t')\n",
    "\n",
    "activations_distance(m0_D_f_activations,ntk_D_f_activations,'Retrain_NTK_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,ntk_D_r_activations,'Retrain_NTK_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,ntk_D_t_activations,'Retrain_NTK_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ddebce-af31-4b10-a832-dfba93d3b0f4",
   "metadata": {},
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217c0cd-6e1c-4720-bb43-4823e93aa97a",
   "metadata": {},
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f09710-8f96-4e04-9e4a-d0121b3ca59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    activations=[]\n",
    "\n",
    "    predictions=[]\n",
    "\n",
    "    if use_bn:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            target=(2*target-1)\n",
    "\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            data=data.view(data.shape[0],-1)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        if scrub_act:\n",
    "\n",
    "            G = []\n",
    "\n",
    "            for cls in range(num_classes):\n",
    "\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "                G.append(grads)\n",
    "\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "\n",
    "            G = torch.stack(G).pow(2)\n",
    "\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "\n",
    "        if samples_correctness:\n",
    "\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "\n",
    "            predictions.append(get_error(output,target))\n",
    "\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "\n",
    "    if samples_correctness:\n",
    "\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e460d-92d4-468c-9fd7-38fd2f1c233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "\n",
    "    l2_loss = 0\n",
    "\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "\n",
    "        if p.requires_grad:\n",
    "\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "\n",
    "    return l2_loss\n",
    "\n",
    "\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader,\n",
    "\n",
    "                    loss_fn: nn.Module,\n",
    "\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "\n",
    "\n",
    "\n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "\n",
    "            input, target = batch\n",
    "\n",
    "            output = model(input)\n",
    "\n",
    "            if split=='test' and scrub_act:\n",
    "\n",
    "                G = []\n",
    "\n",
    "                for cls in range(num_classes):\n",
    "\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "                    G.append(grads)\n",
    "\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "\n",
    "                G = torch.stack(G).pow(2)\n",
    "\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "\n",
    "\n",
    "\n",
    "            if split != 'test':\n",
    "\n",
    "                model.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "    if not quiet:\n",
    "\n",
    "        log_metrics(split, metrics, epoch)\n",
    "\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2f051-e8f0-4d1a-982c-3b91c3819528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "\n",
    "    model_init=copy.deepcopy(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "\n",
    "\n",
    "\n",
    "def test(model, data_loader):\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_init=copy.deepcopy(model)\n",
    "\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a93d3c-b055-4c87-a035-29b0b64d25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.01, epochs=100, threshold=0.01, quiet=True):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = copy.deepcopy(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    model_init=copy.deepcopy(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "\n",
    "            break\n",
    "\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "\n",
    "    return epoch, metrics\n",
    "\n",
    "\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "\n",
    "    print(f\"{name} ->\"\n",
    "\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9831a4-e369-43d7-8e44-cf3209c886d4",
   "metadata": {},
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925511fa-2c1a-4858-b000-b5bbb64480a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9d585-0f22-4595-9987-856a7c81db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        p.grad_acc = 0\n",
    "\n",
    "        p.grad2_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            for p in model.parameters():\n",
    "\n",
    "                if p.requires_grad:\n",
    "\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        p.grad_acc /= len(train_loader)\n",
    "\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7163f1-f673-4ed6-be80-f22027cf016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635213fe-9600-42f7-89c5-16c1c146d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "\n",
    "    var = var.clamp(max=1e3)\n",
    "\n",
    "    if p.size(0) == num_classes:\n",
    "\n",
    "        var = var.clamp(max=1e2)\n",
    "\n",
    "    var = alpha * var\n",
    "\n",
    "\n",
    "\n",
    "    if p.ndim > 1:\n",
    "\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "\n",
    "    if not is_base_dist:\n",
    "\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    else:\n",
    "\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "\n",
    "        mu[class_to_forget] = 0\n",
    "\n",
    "        var[class_to_forget] = 0.0001\n",
    "\n",
    "    if p.size(0) == num_classes:\n",
    "\n",
    "        # Last layer\n",
    "\n",
    "        var *= 10\n",
    "\n",
    "    elif p.ndim == 1:\n",
    "\n",
    "        # BatchNorm\n",
    "\n",
    "        var *= 10\n",
    "\n",
    "#         var*=1\n",
    "\n",
    "    return mu, var\n",
    "\n",
    "\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad9b99-9f09-4a3c-8ec1-96cfbcc70222",
   "metadata": {},
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174be81-8046-4c08-8aed-1cfb11bc28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "\n",
    "alpha = 1e-6\n",
    "\n",
    "total_kl = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "\n",
    "    total_kl += kl\n",
    "\n",
    "    print(k, f'{kl:.1f}')\n",
    "\n",
    "print(\"Total:\", total_kl)\n",
    "\n",
    "log_dict['fisher_info']=total_kl\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb608b9-3c4a-412c-bcd1-1d494c0f79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "\n",
    "alpha = 1e-6\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fc614-f197-46cc-b9a1-4bd13d57af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "\n",
    "print(test(modelf, forget_loader))\n",
    "\n",
    "print(test(modelf, valid_loader_full))\n",
    "\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bd635-bc2d-4d02-ba70-e1ef1ff7f00e",
   "metadata": {},
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a68d0a-3231-4311-8290-f16020ada4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(model0,retain_loader,'Retrain_Model_D_r')\n",
    "\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(model0,forget_loader,'Retrain_Model_D_f')\n",
    "\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(model0,test_loader_full,'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef995c1e-3255-4867-8b56-8eea06b7e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(modelf,retain_loader,'Fisher_D_r')\n",
    "\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(modelf,forget_loader,'Fisher_D_f')\n",
    "\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(modelf,test_loader_full,'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc972be5-6bff-4d8b-a8fe-3f958f439c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,fisher_D_f_predictions,'Retrain_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_f_activations,fisher_D_f_activations,'Retrain_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,fisher_D_r_activations,'Retrain_Fisher_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,fisher_D_t_activations,'Retrain_Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3c31f-8900-4835-8438-50e52ecae479",
   "metadata": {},
   "source": [
    "## NTK + Fisher Noise in the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea743256-13f4-426a-b15b-b6ba1a3ce0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in itertools.chain(modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data = p.data0.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd8e2b-dbe6-4ffa-a52b-3163a6f46d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "\n",
    "alpha = 1e-9\n",
    "\n",
    "total_kl = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for (k, p), (k0, p0) in zip(model_scrubf.named_parameters(), modelf0.named_parameters()):\n",
    "\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "\n",
    "    total_kl += kl\n",
    "\n",
    "    print(k, f'{kl:.1f}')\n",
    "\n",
    "print(\"Total:\", total_kl)\n",
    "\n",
    "log_dict['ntk_fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fe0f1-2a8a-43e1-b995-5cf316704469",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "for i, p in enumerate(model_scrubf.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea7255-a8fd-40ab-9a40-ea138468b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(model_scrubf, retain_loader))\n",
    "\n",
    "print(test(model_scrubf, forget_loader))\n",
    "\n",
    "print(test(model_scrubf, valid_loader_full))\n",
    "\n",
    "print(test(model_scrubf, test_loader_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23974730-a025-425d-83ee-e1d608bf5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_fisher_D_r_activations,ntk_fisher_D_r_predictions=activations_predictions(model_scrubf,retain_loader,'NTK_Fisher_D_r')\n",
    "\n",
    "ntk_fisher_D_f_activations,ntk_fisher_D_f_predictions=activations_predictions(model_scrubf,forget_loader,'NTK_Fisher_D_f')\n",
    "\n",
    "ntk_fisher_D_t_activations,ntk_fisher_D_t_predictions=activations_predictions(model_scrubf,test_loader_full,'NTK_Fisher_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6d07f-0558-457a-bc3b-bfd7ee2b7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,ntk_fisher_D_f_predictions,'Retrain_NTK_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_f_activations,ntk_fisher_D_f_activations,'Retrain_NTK_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,ntk_fisher_D_r_activations,'Retrain_NTK_Fisher_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,ntk_fisher_D_t_activations,'Retrain_NTK_Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f66eb4-7118-48e7-8d65-a76158dd3d66",
   "metadata": {},
   "source": [
    "## Test-error vs Remaining Information in the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b05d43-dd25-48b1-9765-329e3955272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebf54a-89a1-4dfa-b437-06531eac4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77eccd-09ab-41c4-a3e2-a953a439e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [1e-8,1e-7,1e-6,1e-5]\n",
    "\n",
    "test_error_list = []\n",
    "\n",
    "information_list = []\n",
    "\n",
    "\n",
    "\n",
    "runs = 3\n",
    "\n",
    "for s in range(runs):\n",
    "\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "    test_error_list.append([])\n",
    "\n",
    "    information_list.append([])\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "\n",
    "        for i, p in enumerate(model_scrubf.parameters()):\n",
    "\n",
    "            mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "            p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "\n",
    "\n",
    "        for i, p in enumerate(modelf0.parameters()):\n",
    "\n",
    "            mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "            p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "        metrics = test(model_scrubf, test_loader_full)\n",
    "\n",
    "\n",
    "\n",
    "        total_kl = 0\n",
    "\n",
    "        for (k, p), (k0, p0) in zip(model_scrubf.named_parameters(), modelf0.named_parameters()):\n",
    "\n",
    "            mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "            mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "\n",
    "            kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "\n",
    "            total_kl += kl\n",
    "\n",
    "\n",
    "\n",
    "        test_error_list[s].append(metrics['error'])\n",
    "\n",
    "        information_list[s].append(total_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287267a1-8806-416a-aa73-e9412cdb1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [1e-8,1e-7,1e-6,1e-5]\n",
    "\n",
    "alpha_list = np.ndarray.flatten(np.array([alpha_list for i in range(runs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51cb7e3-7034-4548-afce-623ba5cd47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_list = np.ndarray.flatten(np.array(test_error_list))\n",
    "\n",
    "information_list = np.ndarray.flatten(np.array(information_list))info_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381b9b5-fde8-4c99-8e0d-86e3e2e1784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict['alpha'] = alpha_list\n",
    "\n",
    "info_dict['error'] = test_error_list*100\n",
    "\n",
    "info_dict['info'] = information_list\n",
    "\n",
    "df = pd.DataFrame(info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112b45e-fa99-43ed-a33c-bec74b00074f",
   "metadata": {},
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec5e43-ac8f-40b9-b8e4-409876a247ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55685c6-d36e-42b5-b371-5058a1b21ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e2b40-e03a-49d5-a02b-13ece239709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "\n",
    "\n",
    "    model_scrubf.eval()\n",
    "\n",
    "    modelf0.eval()\n",
    "\n",
    "\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "\n",
    "\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "\n",
    "        input, target = batch\n",
    "\n",
    "\n",
    "\n",
    "        output_sf = model_scrubf(input)\n",
    "\n",
    "        G_sf = []\n",
    "\n",
    "\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "            G_sf.append(grads)\n",
    "\n",
    "\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "\n",
    "\n",
    "\n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "\n",
    "        G_sf = G_sf.pow(2)\n",
    "\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "\n",
    "        G_m0 = []\n",
    "\n",
    "\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "\n",
    "\n",
    "\n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "\n",
    "\n",
    "\n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "\n",
    "\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "\n",
    "\n",
    "\n",
    "        loss = loss_fn(output_sf, target)\n",
    "\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "\n",
    "\n",
    "\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02424d3-e8f7-4f07-bb00-412107bfaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance(model1,model2,alpha,seed=1):\n",
    "\n",
    "\n",
    "\n",
    "    delta_w_s = []\n",
    "\n",
    "    delta_w_m0 = []\n",
    "\n",
    "\n",
    "\n",
    "    for i, (k,p) in enumerate(model1.named_parameters()):\n",
    "\n",
    "        mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "        delta_w_s.append(var.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "    for i, (k,p) in enumerate(model2.named_parameters()):\n",
    "\n",
    "        mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "        delta_w_m0.append(var.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "    return torch.cat(delta_w_s), torch.cat(delta_w_m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a649c-7fbe-43fe-980a-5a0c729a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance(model1,model2,alpha,seed=1):\n",
    "\n",
    "\n",
    "\n",
    "    delta_w_s = []\n",
    "\n",
    "    delta_w_m0 = []\n",
    "\n",
    "\n",
    "\n",
    "    for i, (k,p) in enumerate(model1.named_parameters()):\n",
    "\n",
    "        mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "        delta_w_s.append(var.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "    for i, (k,p) in enumerate(model2.named_parameters()):\n",
    "\n",
    "        mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "        delta_w_m0.append(var.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "    return torch.cat(delta_w_s), torch.cat(delta_w_m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f331fe1-277b-49bc-baa1-df7566a73406",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [1e-8,1e-7,1e-6,1e-5]\n",
    "\n",
    "alpha_list = np.ndarray.flatten(np.array([alpha_list for i in range(runs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31feae34-6afc-437b-9102-870952c90556",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_list = np.ndarray.flatten(np.array(test_error_list))\n",
    "\n",
    "information_list = np.ndarray.flatten(np.array(information_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eada88-2eca-4686-806f-c3c6cda370cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict_act = {}\n",
    "\n",
    "info_dict_act['alpha'] = alpha_list\n",
    "\n",
    "info_dict_act['error'] = test_error_list*100\n",
    "\n",
    "info_dict_act['info'] = information_list\n",
    "\n",
    "df_act = pd.DataFrame(info_dict_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b0285-660a-4cb4-b8b0-5c4bd7e894e9",
   "metadata": {},
   "source": [
    "### Information in Activations for Different Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1127e5-d3b9-4591-9c4d-30332d6eec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92c1a2-9d36-48c6-880a-4dea4dd00177",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e971153-f8b1-4943-8e62-729ce550de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets={}\n",
    "\n",
    "datasets['Forget_Set']=forget_loader\n",
    "\n",
    "datasets['Retain_Set']=retain_loader\n",
    "\n",
    "datasets['Test_Set']=test_loader_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbecfc5-d2c3-4068-be7c-d95ffdb9ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "information_list = []\n",
    "\n",
    "runs=1\n",
    "\n",
    "for s in range(runs):\n",
    "\n",
    "    for k in datasets.keys():\n",
    "\n",
    "        delta_w_s, delta_w_m0 = get_variance(model_scrubf, modelf0, 1e-8)\n",
    "\n",
    "        metrics = test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, datasets[k],seed=s)\n",
    "\n",
    "        information_list.append(metrics['kl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3855e3-5400-4fd4-9893-820f2ccb09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['Forget Set' ,'Retain Set', 'Test Set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a5167-4322-4f19-a2cc-1ca808639ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_info(ax,df,information_list,title,no_barplot):\n",
    "\n",
    "\n",
    "\n",
    "    if no_barplot:\n",
    "\n",
    "        sns.lineplot(x=\"info\", y=\"error\",data=df,ci='sd',ax=ax)\n",
    "\n",
    "        ax.set(xscale=\"log\")#,yscale='log')\n",
    "\n",
    "        ax.set_xlabel('Remaining Information (NATs)',size=16)\n",
    "\n",
    "        ax.set_ylabel('Test Error (%)',size=16)\n",
    "\n",
    "        ax.set_title(title,size=16)\n",
    "\n",
    "        ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "        ax.tick_params(axis=\"x\", labelsize=16)\n",
    "\n",
    "    else:\n",
    "\n",
    "        y_pos = range(len(information_list))\n",
    "\n",
    "        ax.grid(zorder=0)\n",
    "\n",
    "        ax.xaxis.grid(False)\n",
    "\n",
    "        ax.yaxis.grid(True)\n",
    "\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "        ax.bar(y_pos, information_list, align='center', color=matplotlib.cm.get_cmap('tab10')(0.95), width=0.5,capsize=5)\n",
    "\n",
    "        ax.set_title('Information in Activations',size=18)\n",
    "\n",
    "        ax.set_facecolor('whitesmoke')\n",
    "\n",
    "        ax.tick_params(axis=\"y\", labelsize=18)\n",
    "\n",
    "        ax.set_xticks(y_pos)\n",
    "\n",
    "        ax.set_xticklabels(labels=labels, size=18, rotation=45)\n",
    "\n",
    "        ylabel='NATs'\n",
    "\n",
    "        ax.set_ylabel(ylabel,size=18)\n",
    "\n",
    "        ax.set_ylim(bottom=-0.001)\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_facecolor(np.array([231,231,240])/256)#'whitesmoke')\n",
    "\n",
    "    ax.grid(color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edaf6cb-b4d5-45e4-9d97-676ebe2baa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(1+3*4.5,4))\n",
    "\n",
    "plot_info(ax[0],df,None,'Information in Weights',True)\n",
    "\n",
    "plot_info(ax[1],df_act,None,'Information in Activations',True)\n",
    "\n",
    "plot_info(ax[2],None,information_list,'Information in Activations',False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('Plots/information.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e489f-e388-4615-b85b-49021b493787",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b081958-faec-4b60-a5ca-b31c5e520f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e9f87-9ab2-4e80-bac8-96488ae4891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(model_ft,retain_loader,'Finetune_D_r')\n",
    "\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(model_ft,forget_loader,'Finetune_D_f')\n",
    "\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(model_ft,test_loader_full,'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74dde6-7c7d-4906-a6ea-e9bb900931c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,finetune_D_f_predictions,'Retrain_Finetune_D_f')\n",
    "\n",
    "activations_distance(m0_D_f_activations,finetune_D_f_activations,'Retrain_Finetune_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,finetune_D_r_activations,'Retrain_Finetune_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,finetune_D_t_activations,'Retrain_Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd3073-364a-48f4-94a2-65a099e348d3",
   "metadata": {},
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7556e7-9d10-4508-b206-7350ad35ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: readouts\n",
    "\n",
    "except: readouts = {}\n",
    "\n",
    "\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_scrubf),thresh,'NTK_Fisher')\n",
    "\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d66b8-6ce5-4fb4-8d12-24245431019a",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e9b69-c482-4626-a8ac-2af91004add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"logs/{m0_name.split('/')[1].split('.')[0]}.npy\",log_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf270c3-b8a7-4510-a4ea-0abaedee77e3",
   "metadata": {},
   "source": [
    "### Activations along the path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764949f3-7cd9-4e4c-ab4d-5be5ada78b2b",
   "metadata": {},
   "source": [
    "##### Membership Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c21bb-cd1e-4310-ad4e-8c16a2196bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "\n",
    "\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "\n",
    "            data, target = batch\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "\n",
    "    return torch.cat(prob)\n",
    "\n",
    "\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):\n",
    "\n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "\n",
    "\n",
    "\n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "\n",
    "\n",
    "\n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])\n",
    "\n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "\n",
    "    clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "\n",
    "    #clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "\n",
    "    clf.fit(X_r, Y_r)\n",
    "\n",
    "    results = clf.predict(X_f)\n",
    "\n",
    "    return results.mean()\n",
    "\n",
    "\n",
    "\n",
    "def plot_entropy_dist(model, ax, title):\n",
    "\n",
    "    train_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "\n",
    "    indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "\n",
    "    replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(train_loader_full, test_loader_full, model, replaced)\n",
    "\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=ax)\n",
    "\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=ax)\n",
    "\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=ax)\n",
    "\n",
    "    ax.legend(prop={'size': 14})\n",
    "\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    ax.set_title(title,size=18)\n",
    "\n",
    "    ax.set_xlabel('Log of Entropy',size=14)\n",
    "\n",
    "    ax.set_ylim(0,0.4)\n",
    "\n",
    "    ax.set_xlim(-35,2)\n",
    "\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model):\n",
    "\n",
    "    prob = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "\n",
    "    print(\"Attack prob: \", prob)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b223b49-4e2b-4010-bde9-005c534d01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_dict={}\n",
    "\n",
    "attack_dict['Original']= membership_attack(retain_loader,forget_loader,test_loader_full,model)\n",
    "\n",
    "attack_dict['Retrain']=membership_attack(retain_loader,forget_loader,test_loader_full,model0)\n",
    "\n",
    "attack_dict['NTK']=membership_attack(retain_loader,forget_loader,test_loader_full,model_scrub)\n",
    "\n",
    "attack_dict['Fisher']=membership_attack(retain_loader,forget_loader,test_loader_full,modelf)\n",
    "\n",
    "attack_dict['Finetune']=membership_attack(retain_loader,forget_loader,test_loader_full,model_ft)\n",
    "\n",
    "attack_dict['Fisher_NTK']=membership_attack(retain_loader,forget_loader,test_loader_full,model_scrubf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
