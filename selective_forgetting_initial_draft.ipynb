{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    " \n",
    "# Returns the number of\n",
    "# objects it has collected\n",
    "# and deallocated\n",
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/d-m-silva/SelectiveForgetting/blob/master/Forgetting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:44:51.506074Z",
     "iopub.status.busy": "2025-01-27T17:44:51.505465Z",
     "iopub.status.idle": "2025-01-27T17:44:53.429150Z",
     "shell.execute_reply": "2025-01-27T17:44:53.427360Z",
     "shell.execute_reply.started": "2025-01-27T17:44:51.506019Z"
    },
    "id": "BK7XrhEShBNC",
    "outputId": "972d92f8-0cd3-4f53-e3f1-0167cd041e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SelectiveForgetting'...\n",
      "remote: Enumerating objects: 56, done.\u001b[K\n",
      "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
      "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
      "remote: Total 56 (delta 25), reused 16 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (56/56), 99.34 KiB | 1.84 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/d-m-silva/SelectiveForgetting.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T17:44:53.431654Z",
     "iopub.status.busy": "2025-01-27T17:44:53.431131Z",
     "iopub.status.idle": "2025-01-27T17:44:53.440071Z",
     "shell.execute_reply": "2025-01-27T17:44:53.438906Z",
     "shell.execute_reply.started": "2025-01-27T17:44:53.431586Z"
    },
    "id": "QUPo029fhNV-",
    "outputId": "697a8e35-e3fa-4300-97f3-739247f0279f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/SelectiveForgetting\n"
     ]
    }
   ],
   "source": [
    "#%cd SelectiveForgetting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2F_c7Z-BgkmF"
   },
   "source": [
    "Code for papers:\n",
    "\n",
    "1) Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations (https://arxiv.org/abs/2003.02960) <br>\n",
    "\n",
    "2) Eternal Sunshine of the Spotless Net : Selective forgetting in Deep Networks (https://arxiv.org/abs/1911.04933)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uK7YzAJcgkmH",
    "outputId": "a27ac439-637c-4a85-d229-c778198f1a11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duart\\AppData\\Local\\Temp\\ipykernel_1040\\2905006116.py:45: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import variational\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import itertools\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from models import *\n",
    "\n",
    "import models\n",
    "\n",
    "from logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6UUkI9PbgkmJ"
   },
   "outputs": [],
   "source": [
    "def pdb():\n",
    "\n",
    "    import pdb\n",
    "\n",
    "    pdb.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gyIehLOXgkmJ"
   },
   "outputs": [],
   "source": [
    "def parameter_count(model):\n",
    "\n",
    "    count=0\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        count+=np.prod(np.array(list(p.shape)))\n",
    "\n",
    "    print(f'Total Number of Parameters: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7VRYBnZSgkmK"
   },
   "outputs": [],
   "source": [
    "def vectorize_params(model):\n",
    "\n",
    "    param = []\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        param.append(p.data.view(-1).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xoEAJGmSgkmK"
   },
   "outputs": [],
   "source": [
    "def print_param_shape(model):\n",
    "\n",
    "    for k,p in model.named_parameters():\n",
    "\n",
    "        print(k,p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLXEiQtigkmK"
   },
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr-KOSSogkmK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%run main.py --dataset lacuna100 --model allcnn --filters 1.0 --lr 0.1 --lossfn ce --num-classes 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsObYYLYiliP"
   },
   "outputs": [],
   "source": [
    "#1) Pretraining on small_cifar10:\n",
    "\n",
    "%run main.py --dataset small_cifar10 --model resnet --filters 0.4 --lr 0.1 --batch-size 128 --lossfn ce --weight-decay 0.0005 --seed 1\n",
    "\n",
    "#2) Pretraining on cifar100:\n",
    "\n",
    "#%run main.py --dataset cifar100 --model resnet --filters 0.4 --lr 0.1 --batch-size 128 --lossfn ce --weight-decay 0.0005 --seed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owjerYaVgkmK"
   },
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls checkpoints\n",
    "\n",
    "#to check a given checkpoint s1 -> !ls {s1}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmrXYx3ugkmL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TRAINING ON small_cifar_5 !!! WITHOUT FORGETTING !!!\n",
    "\n",
    "#1) Using pretained small_cifar10 as checkpoint -> lighter\n",
    "\n",
    "%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --seed 1\n",
    "\n",
    "#2) Using pretained CIFAR100 as checkpoint -> heavier\n",
    "\n",
    "#%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --seed 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boDo9LXPgkmL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TRAINING ON small_cifar_5 !!! WITH FORGETTING !!!\n",
    "\n",
    "#1) Using pretained small_cifar10 as checkpoint -> lighter\n",
    "\n",
    "%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/small_cifar10_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --forget-class 0 --num-to-forget 25 --seed 1\n",
    "\n",
    "#2) Using pretained CIFAR100 as checkpoint -> heavier\n",
    "\n",
    "#%run main.py --dataset small_cifar5 --model resnet --filters 0.4 --lr 0.01 --resume checkpoints/cifar100_resnet_0_4_forget_None_lr_0_1_bs_128_ls_ce_wd_0_0005_seed_1_25.pt --disable-bn --weight-decay 0.1 --batch-size 128 --epochs 31 --forget-class 0 --num-to-forget 25 --seed 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ei83P1MNgkmL"
   },
   "source": [
    "#### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaoE-VV8gkmL"
   },
   "outputs": [],
   "source": [
    "log_dict={}\n",
    "\n",
    "training_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUppGg12gkmL"
   },
   "outputs": [],
   "source": [
    "log_dict['epoch']=training_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55PuzHY-gkmM",
    "outputId": "60930972-167b-4e7d-c042-f2849f353cb2"
   },
   "outputs": [],
   "source": [
    "parameter_count(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaKdKlligkmM"
   },
   "source": [
    "### Loads checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OELtub-YgkmM",
    "outputId": "b757d340-bca4-4e9a-edc4-857bfb37fa37"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model0 = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "\n",
    "arch = args.model\n",
    "\n",
    "filters=args.filters\n",
    "\n",
    "arch_filters = arch +'_'+ str(filters).replace('.','_')\n",
    "\n",
    "augment = False\n",
    "\n",
    "dataset = args.dataset\n",
    "\n",
    "class_to_forget = args.forget_class\n",
    "\n",
    "init_checkpoint = f\"checkpoints/{args.name}_init.pt\"\n",
    "\n",
    "num_classes=args.num_classes\n",
    "\n",
    "num_to_forget = args.num_to_forget\n",
    "\n",
    "num_total = len(train_loader.dataset)\n",
    "\n",
    "num_to_retain = num_total - num_to_forget\n",
    "\n",
    "seed = args.seed\n",
    "\n",
    "unfreeze_start = None\n",
    "\n",
    "\n",
    "\n",
    "learningrate=f\"lr_{str(args.lr).replace('.','_')}\"\n",
    "\n",
    "batch_size=f\"_bs_{str(args.batch_size)}\"\n",
    "\n",
    "lossfn=f\"_ls_{args.lossfn}\"\n",
    "\n",
    "wd=f\"_wd_{str(args.weight_decay).replace('.','_')}\"\n",
    "\n",
    "seed_name=f\"_seed_{args.seed}_\"\n",
    "\n",
    "\n",
    "\n",
    "num_tag = '' if num_to_forget is None else f'_num_{num_to_forget}'\n",
    "\n",
    "unfreeze_tag = '_' if unfreeze_start is None else f'_unfreeze_from_{unfreeze_start}_'\n",
    "\n",
    "augment_tag = '' if not augment else f'augment_'\n",
    "\n",
    "\n",
    "\n",
    "m_name = f'checkpoints/{dataset}_{arch_filters}_forget_None{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "m0_name = f'checkpoints/{dataset}_{arch_filters}_forget_{class_to_forget}{num_tag}{unfreeze_tag}{augment_tag}{learningrate}{batch_size}{lossfn}{wd}{seed_name}{training_epochs}.pt'\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(m_name))\n",
    "\n",
    "model0.load_state_dict(torch.load(m0_name))\n",
    "\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "model0.cuda()\n",
    "\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()\n",
    "\n",
    "for p in model0.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn-q_4REgkmM"
   },
   "outputs": [],
   "source": [
    "log_dict['args']=args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgtjG3blgkmM"
   },
   "source": [
    "### Distance between w(D) and w(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0MiKPJmgkmM"
   },
   "outputs": [],
   "source": [
    "def distance(model,model0):\n",
    "\n",
    "    distance=0\n",
    "\n",
    "    normalization=0\n",
    "\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "\n",
    "        space='  ' if 'bias' in k else ''\n",
    "\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "\n",
    "        distance+=current_dist\n",
    "\n",
    "        normalization+=current_norm\n",
    "\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HjJHVHOgkmM",
    "outputId": "c87d5598-0233-4677-ab9a-0024f3b61390"
   },
   "outputs": [],
   "source": [
    "log_dict['dist_Original_Retrain']=distance(model,model0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX4VWEkdgkmM"
   },
   "source": [
    "### Distance of w(D) from initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbhBZPkVgkmM"
   },
   "outputs": [],
   "source": [
    "def ntk_init(resume,seed=1):\n",
    "\n",
    "    manual_seed(seed)\n",
    "\n",
    "    model_init = models.get_model(arch, num_classes=num_classes, filters_percentage=filters).to(args.device)\n",
    "\n",
    "    model_init.load_state_dict(torch.load(resume))\n",
    "\n",
    "    return model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtImZVkhgkmM",
    "outputId": "0ae53d02-a02f-40e4-8b44-54cb1d59a41f"
   },
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "for p in model_init.parameters():\n",
    "\n",
    "    p.data0 = p.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5vMeGQEgkmM",
    "outputId": "1fbf7b2a-3069-4ff0-c6aa-8043d66ee4cc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_dict['dist_Original_Original_init']=distance(model_init,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43uv9LFggkmN"
   },
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7Zou_W_gkmN",
    "outputId": "688967ad-4295-49c7-829a-a36c4db3ae30"
   },
   "outputs": [],
   "source": [
    "train_loader_full, valid_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=args.batch_size, seed=seed, augment=False, shuffle=True)\n",
    "\n",
    "marked_loader, _, _ = datasets.get_loaders(dataset, class_to_replace=class_to_forget, num_indexes_to_replace=num_to_forget, only_mark=True, batch_size=1, seed=seed, augment=False, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "def replace_loader_dataset(data_loader, dataset, batch_size=args.batch_size, seed=1, shuffle=True):\n",
    "\n",
    "    manual_seed(seed)\n",
    "\n",
    "    loader_args = {'num_workers': 0, 'pin_memory': False}\n",
    "\n",
    "    def _init_fn(worker_id):\n",
    "\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers=0,pin_memory=True,shuffle=shuffle)\n",
    "\n",
    "\n",
    "\n",
    "forget_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "\n",
    "marked = forget_dataset.targets < 0\n",
    "\n",
    "forget_dataset.data = forget_dataset.data[marked]\n",
    "\n",
    "forget_dataset.targets = - forget_dataset.targets[marked] - 1\n",
    "\n",
    "forget_loader = replace_loader_dataset(train_loader_full, forget_dataset, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "retain_dataset = copy.deepcopy(marked_loader.dataset)\n",
    "\n",
    "marked = retain_dataset.targets >= 0\n",
    "\n",
    "retain_dataset.data = retain_dataset.data[marked]\n",
    "\n",
    "retain_dataset.targets = retain_dataset.targets[marked]\n",
    "\n",
    "retain_loader = replace_loader_dataset(train_loader_full, retain_dataset, seed=seed, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "assert(len(forget_dataset) + len(retain_dataset) == len(train_loader_full.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the checkpoints to clear some memory except init_checkpoint (used after!!!!)\n",
    "\n",
    "checkpoints_folder = os.path.dirname(init_checkpoint)\n",
    "\n",
    "files = os.listdir(checkpoints_folder)\n",
    "\n",
    "for file in files:\n",
    "    full_path = os.path.join(checkpoints_folder, file)\n",
    "    if full_path != init_checkpoint:\n",
    "        os.remove(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySUmgi5jgkmN"
   },
   "source": [
    "# NTK based Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyjlkM59gkmN"
   },
   "source": [
    "#### NTK Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykwpUEgngkmN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def delta_w_utils(model_init,dataloader,name='complete'):\n",
    "\n",
    "    model_init.eval()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    G_list = []\n",
    "\n",
    "    f0_minus_y = []\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):#(tqdm(dataloader,leave=False)):\n",
    "\n",
    "        batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "\n",
    "        input, target = batch\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            input = input.view(input.shape[0],-1)\n",
    "\n",
    "        target = target.cpu().detach().numpy()\n",
    "\n",
    "        output = model_init(input)\n",
    "\n",
    "        G_sample=[]\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            grads = torch.autograd.grad(output[0,cls],model_init.parameters(),retain_graph=True)\n",
    "\n",
    "            grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "\n",
    "            G_sample.append(grads)\n",
    "\n",
    "            G_list.append(grads)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            p = output.cpu().detach().numpy().transpose()\n",
    "\n",
    "            #loss_hess = np.eye(len(p))\n",
    "\n",
    "            target = 2*target-1\n",
    "\n",
    "            f0_y_update = p-target\n",
    "\n",
    "        elif args.lossfn=='ce':\n",
    "\n",
    "            p = torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().transpose()\n",
    "\n",
    "            p[target]-=1\n",
    "\n",
    "            f0_y_update = copy.deepcopy(p)\n",
    "\n",
    "        f0_minus_y.append(f0_y_update)\n",
    "\n",
    "    return np.stack(G_list).transpose(),np.vstack(f0_minus_y)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def delta_w_utils(model_init, dataloader, chunk_size=100, name='complete'):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from copy import deepcopy\n",
    "\n",
    "    model_init.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    #G_list = []\n",
    "    #f0_minus_y = []\n",
    "\n",
    "    #Instead of lists, temp files will be generated:\n",
    "\n",
    "    g_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    f0_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "\n",
    "    def process_chunk(chunk):\n",
    "        chunk_G_list = []\n",
    "        chunk_f0_minus_y = []\n",
    "\n",
    "        for batch in chunk:\n",
    "            batch = [tensor.to(next(model_init.parameters()).device) for tensor in batch]\n",
    "            input, target = batch\n",
    "\n",
    "            if 'mnist' in args.dataset:\n",
    "                input = input.view(input.shape[0], -1)\n",
    "\n",
    "            target = target.cpu().detach().numpy()\n",
    "            output = model_init(input)\n",
    "\n",
    "            G_sample = []\n",
    "            for cls in range(num_classes):\n",
    "                #grads = torch.autograd.grad(output[0, cls], model_init.parameters(), retain_graph=True)\n",
    "                #grads = np.concatenate([g.view(-1).cpu().numpy() for g in grads])\n",
    "\n",
    "                grads = torch.autograd.grad(\n",
    "                    output[0, cls], \n",
    "                    model_init.parameters(), \n",
    "                    retain_graph=(cls < num_classes - 1)) #retains grads only when needed\n",
    "\n",
    "                grads = np.concatenate([g.view(-1).cpu().numpy().astype(np.float16) for g in grads])\n",
    "                \n",
    "                G_sample.append(grads)\n",
    "                chunk_G_list.append(grads)\n",
    "\n",
    "            if args.lossfn == 'mse':\n",
    "                p = output.cpu().detach().numpy().transpose()\n",
    "                target = 2 * target - 1\n",
    "                f0_y_update = p - target\n",
    "            elif args.lossfn == 'ce':\n",
    "                p = torch.nn.functional.softmax(output, dim=1).cpu().detach().numpy().transpose()\n",
    "                p[target] -= 1\n",
    "                f0_y_update = deepcopy(p)\n",
    "\n",
    "            chunk_f0_minus_y.append(f0_y_update)\n",
    "\n",
    "            # Cleanup\n",
    "            del grads, G_sample, output, target\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        return chunk_G_list, chunk_f0_minus_y\n",
    "\n",
    "    current_chunk = []\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        current_chunk.append(batch)\n",
    "        if len(current_chunk) == chunk_size or idx == len(dataloader) - 1:\n",
    "            chunk_G_list, chunk_f0_minus_y = process_chunk(current_chunk)\n",
    "\n",
    "            #G_list.extend(chunk_G_list)\n",
    "            #f0_minus_y.extend(chunk_f0_minus_y)\n",
    "\n",
    "            #Instead of lists, using temp files\n",
    "\n",
    "            np.save(g_file, np.array(chunk_G_list))\n",
    "            np.save(f0_file, np.array(chunk_f0_minus_y))\n",
    "            \n",
    "            \n",
    "            current_chunk = []\n",
    "            \n",
    "            print(f\"Processed chunk {idx}, memory usage: {torch.cuda.memory_allocated()} bytes\")\n",
    "\n",
    "            # Cleanup\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    #Code snippet for temp files use-case, if it works:\n",
    "\n",
    "    g_file.close()\n",
    "    f0_file.close()\n",
    "\n",
    "    G_list = np.load(g_file.name, allow_pickle=True)\n",
    "    f0_minus_y = np.load(f0_file.name, allow_pickle=True)\n",
    "\n",
    "    return np.stack(G_list).transpose(), np.vstack(f0_minus_y)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec1jPc17gkmN"
   },
   "source": [
    "#### Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VT3iy_1gkmN",
    "outputId": "156b5806-9270-4d55-ba2c-faaedc7d81a9"
   },
   "outputs": [],
   "source": [
    "model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "G_r,f0_minus_y_r = delta_w_utils(copy.deepcopy(model),retain_loader,'complete') \n",
    "\n",
    "#Code expects a directory to store the NTK results, \"NTK_data\"\n",
    "\n",
    "output_dir = \"NTK_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Saves results in the created directory\n",
    "\n",
    "np.save(os.path.join(output_dir, 'G_r.npy'), G_r)\n",
    "np.save(os.path.join(output_dir, 'f0_minus_y_r.npy'), f0_minus_y_r)\n",
    "\n",
    "\n",
    "#np.save('NTK_data/G_r.npy',G_r)\n",
    "\n",
    "#np.save('NTK_data/f0_minus_y_r.npy',f0_minus_y_r)\n",
    "\n",
    "del G_r, f0_minus_y_r\n",
    "\n",
    "\n",
    "\n",
    "#model_init = ntk_init(init_checkpoint,args.seed)\n",
    "\n",
    "G_f,f0_minus_y_f = delta_w_utils(copy.deepcopy(model),forget_loader,'retain') \n",
    "\n",
    "\n",
    "np.save('NTK_data/G_f.npy',G_f)\n",
    "\n",
    "np.save('NTK_data/f0_minus_y_f.npy',f0_minus_y_f)\n",
    "\n",
    "del G_f, f0_minus_y_f\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiN4oz_DgkmN"
   },
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "\n",
    "G_f = np.load('NTK_data/G_f.npy')\n",
    "\n",
    "G = np.concatenate([G_r,G_f],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/G.npy',G)\n",
    "\n",
    "del G, G_f, G_r\n",
    "\n",
    "\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "\n",
    "f0_minus_y_f = np.load('NTK_data/f0_minus_y_f.npy')\n",
    "\n",
    "f0_minus_y = np.concatenate([f0_minus_y_r,f0_minus_y_f])\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/f0_minus_y.npy',f0_minus_y)\n",
    "\n",
    "del f0_minus_y, f0_minus_y_r, f0_minus_y_f\n",
    "\n",
    "# Cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8PVES7ggkmN"
   },
   "source": [
    "This only requires access to the gradients and the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f5QyeIbgkmN"
   },
   "source": [
    "### w_lin(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.load('NTK_data/G.npy')\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "del G\n",
    "\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "del theta\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "del G, f0_minus_y, theta_inv, w_complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbUJ3WnJgkmO"
   },
   "source": [
    "\n",
    "\"\"\"\n",
    "G = np.load('NTK_data/G.npy')\n",
    "\n",
    "theta = G.transpose().dot(G) + num_total*args.weight_decay*np.eye(G.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "del G\n",
    "\n",
    "\n",
    "\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "\n",
    "np.save('NTK_data/theta.npy',theta)\n",
    "\n",
    "del theta\n",
    "\n",
    "\n",
    "\n",
    "G = np.load('NTK_data/G.npy')\n",
    "\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "\n",
    "w_complete = -G.dot(theta_inv.dot(f0_minus_y))\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/theta_inv.npy',theta_inv)\n",
    "\n",
    "np.save('NTK_data/w_complete.npy',w_complete)\n",
    "\n",
    "del G, f0_minus_y, theta_inv, w_complete\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load G and f0_minus_y once\n",
    "print(\"Loading G and f0_minus_y...\")\n",
    "G = np.load('NTK_data/G.npy')\n",
    "\n",
    "\n",
    "#Converting G to a float 32bit to reduce memory consumption\n",
    "\n",
    "G = G.astype(np.float32)\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "print(f\"G shape: {G.shape}, f0_minus_y shape: {f0_minus_y.shape}\")\n",
    "\n",
    "# Compute theta\n",
    "\"\"\"\"\n",
    "print(\"Calculating theta...\")\n",
    "theta = G.T @ G + num_total * args.weight_decay * np.eye(G.shape[1])\n",
    "\"\"\"\n",
    "\n",
    "print(\"Calculating theta in chunks...\")\n",
    "\n",
    "chunk_size = 100000  # Process 100 000 rows of G at a time\n",
    "num_chunks = int(np.ceil(G.shape[0] / chunk_size))\n",
    "\n",
    "theta = np.zeros((G.shape[1], G.shape[1]), dtype=np.float32)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, G.shape[0])\n",
    "    print(f\"Processing rows {start}:{end}...\")\n",
    "    \n",
    "    G_chunk = G[start:end, :]\n",
    "    theta += G_chunk.T @ G_chunk  # Accumulate the partial results\n",
    "\n",
    "print(\"Hessian completed\")\n",
    "\n",
    "theta += num_total * args.weight_decay * np.eye(G.shape[1])\n",
    "\n",
    "\n",
    "# Compute theta inverse\n",
    "print(\"Calculating theta inverse...\")\n",
    "theta_inv = np.linalg.inv(theta)\n",
    "\n",
    "# Save theta and theta_inv\n",
    "print(\"Saving theta and theta_inv...\")\n",
    "np.save('NTK_data/theta.npy', theta)\n",
    "np.save('NTK_data/theta_inv.npy', theta_inv)\n",
    "\n",
    "del theta\n",
    "\n",
    "print(\"Calculating w_complete in chunks...\")\n",
    "\n",
    "w_complete = np.zeros((G.shape[0], f0_minus_y.shape[1]), dtype=np.float32)\n",
    "\n",
    "chunk_size = 100000  \n",
    "num_chunks = int(np.ceil(G.shape[0] / chunk_size))\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, G.shape[0])\n",
    "    print(f\"Processing rows {start}:{end}...\")\n",
    "    \n",
    "    G_chunk = G[start:end, :]\n",
    "    w_complete[start:end, :] = -G_chunk @ (theta_inv @ f0_minus_y)\n",
    "\n",
    "print(\"w_complete calculation completed.\")\n",
    "\n",
    "\"\"\"\n",
    "# Compute w_complete\n",
    "print(\"Calculating w_complete...\")\n",
    "w_complete = -G @ (theta_inv @ f0_minus_y)\n",
    "\"\"\"\n",
    "\n",
    "# Save w_complete\n",
    "print(\"Saving w_complete...\")\n",
    "np.save('NTK_data/w_complete.npy', w_complete)\n",
    "\n",
    "# Cleanup\n",
    "print(\"Releasing memory...\")\n",
    "del G, f0_minus_y, theta_inv, w_complete\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtpmsTHfgkmO"
   },
   "source": [
    "### w_lin(D_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "del G_r\n",
    "\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "del theta_r\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4OEeKc8gkmO"
   },
   "source": [
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "\n",
    "#Converting G_r to a float 32bit to reduce memory consumption\n",
    "\n",
    "G_r = G_r.astype(np.float32)\n",
    "f0_minus_y = np.load('NTK_data/f0_minus_y.npy')\n",
    "print(f\"G shape: {G_r.shape}, f0_minus_y shape: {f0_minus_y.shape}\")\n",
    "\n",
    "# Compute theta\n",
    "\n",
    "#theta_r = G_r.transpose().dot(G_r) + num_to_retain*args.weight_decay*np.eye(G_r.shape[1])\n",
    "\n",
    "print(\"Calculating theta in chunks...\")\n",
    "\n",
    "chunk_size = 100000  # Process 100 000 rows of G at a time\n",
    "num_chunks = int(np.ceil(G.shape[0] / chunk_size))\n",
    "\n",
    "theta = np.zeros((G_r.shape[1], G_r.shape[1]), dtype=np.float32)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, G_r.shape[0])\n",
    "    print(f\"Processing rows {start}:{end}...\")\n",
    "    \n",
    "    G_r_chunk = G_r[start:end, :]\n",
    "    theta_r += G_r_chunk.T @ G_r_chunk  # Accumulate the partial results\n",
    "\n",
    "print(\"Hessian completed\")\n",
    "\n",
    "\n",
    "theta_r += num_to_retain * args.weight_decay * np.eye(G_r.shape[1])\n",
    "\n",
    "del G_r\n",
    "\n",
    "print(\"Theta_r was calculated!!!\")\n",
    "\n",
    "theta_r_inv = np.linalg.inv(theta_r)\n",
    "\n",
    "np.save('NTK_data/theta_r.npy',theta_r)\n",
    "\n",
    "del theta_r\n",
    "\n",
    "print(\"Theta_r inverse was calculated!!!\")\n",
    "\n",
    "G_r = np.load('NTK_data/G_r.npy')\n",
    "\n",
    "f0_minus_y_r = np.load('NTK_data/f0_minus_y_r.npy')\n",
    "\n",
    "print(\"Calculating w_retain in chunks...\")\n",
    "\n",
    "#w_retain = -G_r.dot(theta_r_inv.dot(f0_minus_y_r))\n",
    "\n",
    "w_retain = np.zeros((G_r.shape[0], f0_minus_y_r.shape[1]), dtype=np.float32)\n",
    "\n",
    "chunk_size = 100000  \n",
    "num_chunks = int(np.ceil(G_r.shape[0] / chunk_size))\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, G_r.shape[0])\n",
    "    print(f\"Processing rows {start}:{end}...\")\n",
    "    \n",
    "    G_r_chunk = G_r[start:end, :]\n",
    "    w_retain[start:end, :] = -G_r_chunk @ (theta_r_inv @ f0_minus_y_r)\n",
    "\n",
    "print(\"w_retain calculation completed.\")\n",
    "\n",
    "\n",
    "\n",
    "np.save('NTK_data/theta_r_inv.npy',theta_r_inv)\n",
    "\n",
    "np.save('NTK_data/w_retain.npy',w_retain)\n",
    "\n",
    "#cleanup\n",
    "\n",
    "del G_r, f0_minus_y_r, theta_r_inv, w_retain\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFJFmWiUgkmO"
   },
   "source": [
    "#### Scrubbing Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTmgTyYDgkmP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Scrubbing Direction\n",
    "\n",
    "w_complete = np.load('NTK_data/w_complete.npy')\n",
    "\n",
    "w_retain = np.load('NTK_data/w_retain.npy')\n",
    "\n",
    "delta_w = (w_retain-w_complete).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo26CjC5gkmP"
   },
   "outputs": [],
   "source": [
    "delta_w_copy = copy.deepcopy(delta_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkjYbsK1gkmP"
   },
   "source": [
    "#### Actual Change in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qURGuTxgkmP"
   },
   "outputs": [],
   "source": [
    "delta_w_actual = vectorize_params(model0)-vectorize_params(model)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Actual Norm-: {np.linalg.norm(delta_w_actual)}')\n",
    "\n",
    "print(f'Predtn Norm-: {np.linalg.norm(delta_w)}')\n",
    "\n",
    "scale_ratio = np.linalg.norm(delta_w_actual)/np.linalg.norm(delta_w)\n",
    "\n",
    "print('Actual Scale: {}'.format(scale_ratio))\n",
    "\n",
    "log_dict['actual_scale_ratio']=scale_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUq-Wj0PgkmP"
   },
   "source": [
    "#### Trapezium Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXzFogxmgkmP"
   },
   "outputs": [],
   "source": [
    "m_pred_error = vectorize_params(model)-vectorize_params(model_init)-w_retain.squeeze()\n",
    "\n",
    "print(f\"Delta w -------: {np.linalg.norm(delta_w)}\")\n",
    "\n",
    "\n",
    "\n",
    "inner = np.inner(delta_w/np.linalg.norm(delta_w),m_pred_error/np.linalg.norm(m_pred_error))\n",
    "\n",
    "print(f\"Inner Product--: {inner}\")\n",
    "\n",
    "\n",
    "\n",
    "if inner<0:\n",
    "\n",
    "    angle = np.arccos(inner)-np.pi/2\n",
    "\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.sin(angle)*np.linalg.norm(m_pred_error)\n",
    "\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "else:\n",
    "\n",
    "    angle = np.arccos(inner)\n",
    "\n",
    "    print(f\"Angle----------:  {angle}\")\n",
    "\n",
    "\n",
    "\n",
    "    predicted_norm=np.linalg.norm(delta_w) + 2*np.cos(angle)*np.linalg.norm(m_pred_error)\n",
    "\n",
    "    print(f\"Pred Act Norm--:  {predicted_norm}\")\n",
    "\n",
    "\n",
    "\n",
    "predicted_scale=predicted_norm/np.linalg.norm(delta_w)\n",
    "\n",
    "predicted_scale\n",
    "\n",
    "print(f\"Predicted Scale:  {predicted_scale}\")\n",
    "\n",
    "log_dict['predicted_scale_ratio']=predicted_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuAFLO_fgkmP"
   },
   "source": [
    "#### Normalized Inner Product between Prediction and Actual Scrubbing Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMT8dOA5gkmQ"
   },
   "outputs": [],
   "source": [
    "def NIP(v1,v2):\n",
    "\n",
    "    nip = (np.inner(v1/np.linalg.norm(v1),v2/np.linalg.norm(v2)))\n",
    "\n",
    "    print(nip)\n",
    "\n",
    "    return nip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woNDjfHfgkmQ",
    "outputId": "f578d34a-8ae2-4a1b-fffb-560c6e1cc82a"
   },
   "outputs": [],
   "source": [
    "nip=NIP(delta_w_actual,delta_w)\n",
    "\n",
    "log_dict['nip']=nip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wFnFo7ZgkmQ"
   },
   "source": [
    "#### Reshape delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3AlUMJNgkmQ"
   },
   "outputs": [],
   "source": [
    "def get_delta_w_dict(delta_w,model):\n",
    "\n",
    "    # Give normalized delta_w\n",
    "\n",
    "    delta_w_dict = OrderedDict()\n",
    "\n",
    "    params_visited = 0\n",
    "\n",
    "    for k,p in model.named_parameters():\n",
    "\n",
    "        num_params = np.prod(list(p.shape))\n",
    "\n",
    "        update_params = delta_w[params_visited:params_visited+num_params]\n",
    "\n",
    "        delta_w_dict[k] = torch.Tensor(update_params).view_as(p)\n",
    "\n",
    "        params_visited+=num_params\n",
    "\n",
    "    return delta_w_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTHubJ7kgkmQ"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehqehjAqgkmQ"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    activations=[]\n",
    "\n",
    "    predictions=[]\n",
    "\n",
    "    if use_bn:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            target=(2*target-1)\n",
    "\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            data=data.view(data.shape[0],-1)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "\n",
    "        if samples_correctness:\n",
    "\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "\n",
    "            predictions.append(get_error(output,target))\n",
    "\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "\n",
    "    if samples_correctness:\n",
    "\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOyCz_aYgkmT"
   },
   "outputs": [],
   "source": [
    "def activations_predictions(model,dataloader,name):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics,activations,predictions=get_metrics(model,dataloader,criterion,True)\n",
    "\n",
    "    print(f\"{name} -> Loss:{np.round(metrics['loss'],3)}, Error:{metrics['error']}\")\n",
    "\n",
    "    log_dict[f\"{name}_loss\"]=metrics['loss']\n",
    "\n",
    "    log_dict[f\"{name}_error\"]=metrics['error']\n",
    "\n",
    "    return activations,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgRx5C8AgkmT"
   },
   "outputs": [],
   "source": [
    "def predictions_distance(l1,l2,name):\n",
    "\n",
    "    dist = np.sum(np.abs(l1-l2))\n",
    "\n",
    "    print(f\"Predictions Distance {name} -> {dist}\")\n",
    "\n",
    "    log_dict[f\"{name}_predictions\"]=dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tclizE_YgkmT"
   },
   "outputs": [],
   "source": [
    "def activations_distance(a1,a2,name):\n",
    "\n",
    "    dist = np.linalg.norm(a1-a2,ord=1,axis=1).mean()\n",
    "\n",
    "    print(f\"Activations Distance {name} -> {dist}\")\n",
    "\n",
    "    log_dict[f\"{name}_activations\"]=dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf40R_e0gkmT"
   },
   "source": [
    "### Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MYlPQ_HgkmU",
    "outputId": "f052425e-bc12-4880-923e-705ea015d413"
   },
   "outputs": [],
   "source": [
    "m_D_r_activations,m_D_r_predictions=activations_predictions(model,retain_loader,'Original_Model_D_r')\n",
    "\n",
    "m_D_f_activations,m_D_f_predictions=activations_predictions(model,forget_loader,'Original_Model_D_f')\n",
    "\n",
    "m_D_t_activations,m_D_t_predictions=activations_predictions(model,test_loader_full,'Original_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OVwIrDegkmU"
   },
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQlT4yTRgkmU",
    "outputId": "852f2392-9fc5-4a27-9509-edacb33a3f3f"
   },
   "outputs": [],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(model0,retain_loader,'Retrain_Model_D_r')\n",
    "\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(model0,forget_loader,'Retrain_Model_D_f')\n",
    "\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(model0,test_loader_full,'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqM7FvJUgkmU"
   },
   "source": [
    "# Scrub using NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB603OJagkmU"
   },
   "outputs": [],
   "source": [
    "scale=predicted_scale\n",
    "\n",
    "direction = get_delta_w_dict(delta_w,model)\n",
    "\n",
    "\n",
    "\n",
    "model_scrub = copy.deepcopy(model)\n",
    "\n",
    "for k,p in model_scrub.named_parameters():\n",
    "\n",
    "    p.data += (direction[k]*scale).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN6qc5IpgkmU"
   },
   "outputs": [],
   "source": [
    "ntk_D_r_activations,ntk_D_r_predictions=activations_predictions(model_scrub,retain_loader,'NTK_D_r')\n",
    "\n",
    "ntk_D_f_activations,ntk_D_f_predictions=activations_predictions(model_scrub,forget_loader,'NTK_D_f')\n",
    "\n",
    "ntk_D_t_activations,ntk_D_t_predictions=activations_predictions(model_scrub,test_loader_full,'NTK_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muBxkQdIgkmU"
   },
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,m_D_f_predictions,'Retrain_Original_D_f')\n",
    "\n",
    "predictions_distance(m0_D_f_predictions,ntk_D_f_predictions,'Retrain_NTK_D_f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I3UKSGXgkmV"
   },
   "source": [
    "### Activations Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC3qF08egkmV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activations_distance(m0_D_f_activations,m_D_f_activations,'Retrain_Original_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,m_D_r_activations,'Retrain_Original_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,m_D_t_activations,'Retrain_Original_D_t')\n",
    "\n",
    "activations_distance(m0_D_f_activations,ntk_D_f_activations,'Retrain_NTK_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,ntk_D_r_activations,'Retrain_NTK_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,ntk_D_t_activations,'Retrain_NTK_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK7BAgUugkmV"
   },
   "source": [
    "# Fisher Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIdkM-2ogkmV"
   },
   "source": [
    "### Finetune and Fisher Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aq2i1PxfgkmV"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "def get_metrics(model,dataloader,criterion,samples_correctness=False,use_bn=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    activations=[]\n",
    "\n",
    "    predictions=[]\n",
    "\n",
    "    if use_bn:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(retain_loader.dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "                data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    mult = 0.5 if args.lossfn=='mse' else 1\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "\n",
    "        if args.lossfn=='mse':\n",
    "\n",
    "            target=(2*target-1)\n",
    "\n",
    "            target = target.type(torch.cuda.FloatTensor).unsqueeze(1)\n",
    "\n",
    "        if 'mnist' in args.dataset:\n",
    "\n",
    "            data=data.view(data.shape[0],-1)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        if scrub_act:\n",
    "\n",
    "            G = []\n",
    "\n",
    "            for cls in range(num_classes):\n",
    "\n",
    "                grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "\n",
    "                grads = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "                G.append(grads)\n",
    "\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "\n",
    "            G = torch.stack(G).pow(2)\n",
    "\n",
    "            delta_f = torch.matmul(G,delta_w)\n",
    "\n",
    "            output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "\n",
    "\n",
    "        loss = mult*criterion(output, target)\n",
    "\n",
    "        if samples_correctness:\n",
    "\n",
    "            activations.append(torch.nn.functional.softmax(output,dim=1).cpu().detach().numpy().squeeze())\n",
    "\n",
    "            predictions.append(get_error(output,target))\n",
    "\n",
    "        metrics.update(n=data.size(0), loss=loss.item(), error=get_error(output, target))\n",
    "\n",
    "    if samples_correctness:\n",
    "\n",
    "        return metrics.avg,np.stack(activations),np.array(predictions)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRhdkXvugkmV"
   },
   "outputs": [],
   "source": [
    "def l2_penalty(model,model_init,weight_decay):\n",
    "\n",
    "    l2_loss = 0\n",
    "\n",
    "    for (k,p),(k_init,p_init) in zip(model.named_parameters(),model_init.named_parameters()):\n",
    "\n",
    "        if p.requires_grad:\n",
    "\n",
    "            l2_loss += (p-p_init).pow(2).sum()\n",
    "\n",
    "    l2_loss *= (weight_decay/2.)\n",
    "\n",
    "    return l2_loss\n",
    "\n",
    "\n",
    "\n",
    "def run_train_epoch(model: nn.Module, model_init, data_loader: torch.utils.data.DataLoader,\n",
    "\n",
    "                    loss_fn: nn.Module,\n",
    "\n",
    "                    optimizer: torch.optim.SGD, split: str, epoch: int, ignore_index=None,\n",
    "\n",
    "                    negative_gradient=False, negative_multiplier=-1, random_labels=False,\n",
    "\n",
    "                    quiet=False,delta_w=None,scrub_act=False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    num_labels = data_loader.dataset.targets.max().item() + 1\n",
    "\n",
    "\n",
    "\n",
    "    with torch.set_grad_enabled(split != 'test'):\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "\n",
    "            input, target = batch\n",
    "\n",
    "            output = model(input)\n",
    "\n",
    "            if split=='test' and scrub_act:\n",
    "\n",
    "                G = []\n",
    "\n",
    "                for cls in range(num_classes):\n",
    "\n",
    "                    grads = torch.autograd.grad(output[0,cls],model.parameters(),retain_graph=True)\n",
    "\n",
    "                    grads = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "                    G.append(grads)\n",
    "\n",
    "                grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "\n",
    "                G = torch.stack(G).pow(2)\n",
    "\n",
    "                delta_f = torch.matmul(G,delta_w)\n",
    "\n",
    "                output += delta_f.sqrt()*torch.empty_like(delta_f).normal_()\n",
    "\n",
    "            loss = loss_fn(output, target) + l2_penalty(model,model_init,args.weight_decay)\n",
    "\n",
    "            metrics.update(n=input.size(0), loss=loss_fn(output,target).item(), error=get_error(output, target))\n",
    "\n",
    "\n",
    "\n",
    "            if split != 'test':\n",
    "\n",
    "                model.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "    if not quiet:\n",
    "\n",
    "        log_metrics(split, metrics, epoch)\n",
    "\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ua92LIGVgkmW"
   },
   "outputs": [],
   "source": [
    "def finetune(model: nn.Module, data_loader: torch.utils.data.DataLoader, lr=0.01, epochs=10, quiet=False):\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "\n",
    "    model_init=copy.deepcopy(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        run_train_epoch(model, model_init, data_loader, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "\n",
    "\n",
    "\n",
    "def test(model, data_loader):\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_init=copy.deepcopy(model)\n",
    "\n",
    "    return run_train_epoch(model, model_init, data_loader, loss_fn, optimizer=None, split='test', epoch=epoch, ignore_index=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nryj-HJ_gkmW"
   },
   "outputs": [],
   "source": [
    "def readout_retrain(model, data_loader, test_loader, lr=0.01, epochs=100, threshold=0.01, quiet=True):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = copy.deepcopy(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "\n",
    "    sampler = torch.utils.data.RandomSampler(data_loader.dataset, replacement=True, num_samples=500)\n",
    "\n",
    "    data_loader_small = torch.utils.data.DataLoader(data_loader.dataset, batch_size=data_loader.batch_size, sampler=sampler, num_workers=data_loader.num_workers)\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    model_init=copy.deepcopy(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        metrics.append(run_train_epoch(model, model_init, test_loader, loss_fn, optimizer, split='test', epoch=epoch, ignore_index=None, quiet=quiet))\n",
    "\n",
    "        if metrics[-1]['loss'] <= threshold:\n",
    "\n",
    "            break\n",
    "\n",
    "        run_train_epoch(model, model_init, data_loader_small, loss_fn, optimizer, split='train', epoch=epoch, ignore_index=None, quiet=quiet)\n",
    "\n",
    "    return epoch, metrics\n",
    "\n",
    "\n",
    "\n",
    "def extract_retrain_time(metrics, threshold=0.1):\n",
    "\n",
    "    losses = np.array([m['loss'] for m in metrics])\n",
    "\n",
    "    return np.argmax(losses < threshold)\n",
    "\n",
    "\n",
    "\n",
    "def all_readouts(model,thresh=0.1,name='method'):\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader_full.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    retrain_time, _ = readout_retrain(model, train_loader, forget_loader, epochs=100, lr=0.01, threshold=thresh)\n",
    "\n",
    "    test_error = test(model, test_loader_full)['error']\n",
    "\n",
    "    forget_error = test(model, forget_loader)['error']\n",
    "\n",
    "    retain_error = test(model, retain_loader)['error']\n",
    "\n",
    "    print(f\"{name} ->\"\n",
    "\n",
    "          f\"\\tFull test error: {test_error:.2%}\"\n",
    "\n",
    "          f\"\\tForget error: {forget_error:.2%}\\tRetain error: {retain_error:.2%}\"\n",
    "\n",
    "          f\"\\tFine-tune time: {retrain_time+1} steps\")\n",
    "\n",
    "    log_dict[f\"{name}_retrain_time\"]=retrain_time+1\n",
    "\n",
    "    return(dict(test_error=test_error, forget_error=forget_error, retain_error=retain_error, retrain_time=retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arQp-qHSgkmW"
   },
   "source": [
    "# Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8H3__QIgkmW"
   },
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Srf_bZngkmX"
   },
   "outputs": [],
   "source": [
    "def hessian(dataset, model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        p.grad_acc = 0\n",
    "\n",
    "        p.grad2_acc = 0\n",
    "\n",
    "\n",
    "\n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "\n",
    "        data, orig_target = data.to(args.device), orig_target.to(args.device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            for p in model.parameters():\n",
    "\n",
    "                if p.requires_grad:\n",
    "\n",
    "                    p.grad_acc += (orig_target == target).float() * p.grad.data\n",
    "\n",
    "                    p.grad2_acc += prob[:, y] * p.grad.data.pow(2)\n",
    "\n",
    "    for p in model.parameters():\n",
    "\n",
    "        p.grad_acc /= len(train_loader)\n",
    "\n",
    "        p.grad2_acc /= len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFCtZgq_gkmX",
    "outputId": "8f717597-5326-4ca7-fb69-1ff2169919e8"
   },
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-lcg4NigkmX"
   },
   "outputs": [],
   "source": [
    "def get_mean_var(p, is_base_dist=False, alpha=3e-6):\n",
    "\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "\n",
    "    var = var.clamp(max=1e3)\n",
    "\n",
    "    if p.size(0) == num_classes:\n",
    "\n",
    "        var = var.clamp(max=1e2)\n",
    "\n",
    "    var = alpha * var\n",
    "\n",
    "\n",
    "\n",
    "    if p.ndim > 1:\n",
    "\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "\n",
    "    if not is_base_dist:\n",
    "\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    else:\n",
    "\n",
    "        mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    if p.size(0) == num_classes and num_to_forget is None:\n",
    "\n",
    "        mu[class_to_forget] = 0\n",
    "\n",
    "        var[class_to_forget] = 0.0001\n",
    "\n",
    "    if p.size(0) == num_classes:\n",
    "\n",
    "        # Last layer\n",
    "\n",
    "        var *= 10\n",
    "\n",
    "    elif p.ndim == 1:\n",
    "\n",
    "        # BatchNorm\n",
    "\n",
    "        var *= 10\n",
    "\n",
    "#         var*=1\n",
    "\n",
    "    return mu, var\n",
    "\n",
    "\n",
    "\n",
    "def kl_divergence_fisher(mu0, var0, mu1, var1):\n",
    "\n",
    "    return ((mu1 - mu0).pow(2)/var0 + var1/var0 - torch.log(var1/var0) - 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qc2Q-4DgkmX"
   },
   "source": [
    "## Fisher Noise in Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNFyLbX6gkmX",
    "outputId": "124c404a-0d6f-4a60-fb33-dcff098b51e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "\n",
    "alpha = 1e-6\n",
    "\n",
    "total_kl = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for (k, p), (k0, p0) in zip(modelf.named_parameters(), modelf0.named_parameters()):\n",
    "\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "\n",
    "    total_kl += kl\n",
    "\n",
    "    print(k, f'{kl:.1f}')\n",
    "\n",
    "print(\"Total:\", total_kl)\n",
    "\n",
    "log_dict['fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcMjaNjygkmY"
   },
   "outputs": [],
   "source": [
    "fisher_dir = []\n",
    "\n",
    "alpha = 1e-6\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for i, p in enumerate(modelf.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "    fisher_dir.append(var.sqrt().view(-1).cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1dAaIXAgkmY",
    "outputId": "52a14ea0-7542-476d-94db-04be0f71a0d8"
   },
   "outputs": [],
   "source": [
    "print(test(modelf, retain_loader))\n",
    "\n",
    "print(test(modelf, forget_loader))\n",
    "\n",
    "print(test(modelf, valid_loader_full))\n",
    "\n",
    "print(test(modelf, test_loader_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_4XNYRNgkmY"
   },
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bigcfyIDgkmY",
    "outputId": "85f52987-df08-4f3b-8be4-a960c0b40e15"
   },
   "outputs": [],
   "source": [
    "m0_D_r_activations,m0_D_r_predictions=activations_predictions(model0,retain_loader,'Retrain_Model_D_r')\n",
    "\n",
    "m0_D_f_activations,m0_D_f_predictions=activations_predictions(model0,forget_loader,'Retrain_Model_D_f')\n",
    "\n",
    "m0_D_t_activations,m0_D_t_predictions=activations_predictions(model0,test_loader_full,'Retrain_Model_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1lf2shGgkmY",
    "outputId": "096aae8d-88fc-4e1c-b1a4-78ecf52e7656"
   },
   "outputs": [],
   "source": [
    "fisher_D_r_activations,fisher_D_r_predictions=activations_predictions(modelf,retain_loader,'Fisher_D_r')\n",
    "\n",
    "fisher_D_f_activations,fisher_D_f_predictions=activations_predictions(modelf,forget_loader,'Fisher_D_f')\n",
    "\n",
    "fisher_D_t_activations,fisher_D_t_predictions=activations_predictions(modelf,test_loader_full,'Fisher_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BhnEok_gkmY",
    "outputId": "13154b98-cd36-4298-8fc1-0940829f9f3d"
   },
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,fisher_D_f_predictions,'Retrain_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_f_activations,fisher_D_f_activations,'Retrain_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,fisher_D_r_activations,'Retrain_Fisher_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,fisher_D_t_activations,'Retrain_Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVxxZ25lgkmZ"
   },
   "source": [
    "## NTK + Fisher Noise in the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6SHa5pmgkmZ"
   },
   "outputs": [],
   "source": [
    "for p in itertools.chain(modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data = p.data0.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYyOktW2gkmZ",
    "outputId": "48eca99a-4971-483b-d8f1-06ad5dad32c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the amount of information not forgotten at all layers using the given alpha\n",
    "\n",
    "alpha = 1e-9\n",
    "\n",
    "total_kl = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for (k, p), (k0, p0) in zip(model_scrubf.named_parameters(), modelf0.named_parameters()):\n",
    "\n",
    "    mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "\n",
    "    kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "\n",
    "    total_kl += kl\n",
    "\n",
    "    print(k, f'{kl:.1f}')\n",
    "\n",
    "print(\"Total:\", total_kl)\n",
    "\n",
    "log_dict['ntk_fisher_info']=total_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oqgj--YhgkmZ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "for i, p in enumerate(model_scrubf.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "\n",
    "\n",
    "for i, p in enumerate(modelf0.parameters()):\n",
    "\n",
    "    mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "    p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkkSA42JgkmZ",
    "outputId": "5298f628-fc32-4d55-957a-266d0b1ac891"
   },
   "outputs": [],
   "source": [
    "print(test(model_scrubf, retain_loader))\n",
    "\n",
    "print(test(model_scrubf, forget_loader))\n",
    "\n",
    "print(test(model_scrubf, valid_loader_full))\n",
    "\n",
    "print(test(model_scrubf, test_loader_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1qSf8NfgkmZ",
    "outputId": "1ac1dc6c-8cee-4fc6-dbeb-86175fe14aef"
   },
   "outputs": [],
   "source": [
    "ntk_fisher_D_r_activations,ntk_fisher_D_r_predictions=activations_predictions(model_scrubf,retain_loader,'NTK_Fisher_D_r')\n",
    "\n",
    "ntk_fisher_D_f_activations,ntk_fisher_D_f_predictions=activations_predictions(model_scrubf,forget_loader,'NTK_Fisher_D_f')\n",
    "\n",
    "ntk_fisher_D_t_activations,ntk_fisher_D_t_predictions=activations_predictions(model_scrubf,test_loader_full,'NTK_Fisher_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-2vD99WgkmZ",
    "outputId": "a94460d6-d584-4bd5-a92a-f3c60b274938"
   },
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,ntk_fisher_D_f_predictions,'Retrain_NTK_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_f_activations,ntk_fisher_D_f_activations,'Retrain_NTK_Fisher_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,ntk_fisher_D_r_activations,'Retrain_NTK_Fisher_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,ntk_fisher_D_t_activations,'Retrain_NTK_Fisher_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSuizaqMgkmZ"
   },
   "source": [
    "## Test-error vs Remaining Information in the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sxtba_cgkma"
   },
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnVClOtlgkma",
    "outputId": "3940a1b0-bf74-4c12-ee20-8d843eedb065"
   },
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApfseOjUgkma"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pTnet3fgkma",
    "outputId": "d0a0e59a-563b-4e7f-d084-40ea880f5f92"
   },
   "outputs": [],
   "source": [
    "alpha_list = [1e-8,1e-7,1e-6,1e-5]\n",
    "\n",
    "test_error_list = []\n",
    "\n",
    "information_list = []\n",
    "\n",
    "\n",
    "\n",
    "runs = 3\n",
    "\n",
    "for s in range(runs):\n",
    "\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "    test_error_list.append([])\n",
    "\n",
    "    information_list.append([])\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "\n",
    "        for i, p in enumerate(model_scrubf.parameters()):\n",
    "\n",
    "            mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "            p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "\n",
    "\n",
    "        for i, p in enumerate(modelf0.parameters()):\n",
    "\n",
    "            mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "            p.data = mu + var.sqrt() * torch.empty_like(p.data0).normal_()\n",
    "\n",
    "        metrics = test(model_scrubf, test_loader_full)\n",
    "\n",
    "\n",
    "\n",
    "        total_kl = 0\n",
    "\n",
    "        for (k, p), (k0, p0) in zip(model_scrubf.named_parameters(), modelf0.named_parameters()):\n",
    "\n",
    "            mu0, var0 = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "            mu1, var1 = get_mean_var(p0, True, alpha=alpha)\n",
    "\n",
    "            kl = kl_divergence_fisher(mu0, var0, mu1, var1).item()\n",
    "\n",
    "            total_kl += kl\n",
    "\n",
    "\n",
    "\n",
    "        test_error_list[s].append(metrics['error'])\n",
    "\n",
    "        information_list[s].append(total_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2VcNTfcgkma"
   },
   "outputs": [],
   "source": [
    "alpha_list = [1e-8,1e-7,1e-6,1e-5]\n",
    "\n",
    "alpha_list = np.ndarray.flatten(np.array([alpha_list for i in range(runs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HznAKdgAgkma"
   },
   "outputs": [],
   "source": [
    "test_error_list = np.ndarray.flatten(np.array(test_error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02naA8esgkma"
   },
   "outputs": [],
   "source": [
    "information_list = np.ndarray.flatten(np.array(information_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2xG0fSUgkma"
   },
   "outputs": [],
   "source": [
    "info_dict = {}\n",
    "\n",
    "info_dict['alpha'] = alpha_list\n",
    "\n",
    "info_dict['error'] = test_error_list*100\n",
    "\n",
    "info_dict['info'] = information_list\n",
    "\n",
    "df = pd.DataFrame(info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D94KS1Pgkma"
   },
   "source": [
    "### Information in the Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNhK2-Cmgkmb"
   },
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kGWyh2Ugkmb",
    "outputId": "c1bb8d0c-9dc5-42bf-d34b-3ff5f586c4e0"
   },
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPWjMw1Agkmb"
   },
   "outputs": [],
   "source": [
    "def test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, data_loader, \\\n",
    "\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\\\n",
    "\n",
    "                    optimizer=torch.optim.SGD, \\\n",
    "\n",
    "                    seed=1,quiet=False):\n",
    "\n",
    "\n",
    "\n",
    "    model_scrubf.eval()\n",
    "\n",
    "    modelf0.eval()\n",
    "\n",
    "\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    metrics = AverageMeter()\n",
    "\n",
    "    num_classes = data_loader.dataset.targets.max().item() + 1\n",
    "\n",
    "\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "\n",
    "        batch = [tensor.to(next(model_scrubf.parameters()).device) for tensor in batch]\n",
    "\n",
    "        input, target = batch\n",
    "\n",
    "\n",
    "\n",
    "        output_sf = model_scrubf(input)\n",
    "\n",
    "        G_sf = []\n",
    "\n",
    "\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=True)\n",
    "\n",
    "            grads = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "            G_sf.append(grads)\n",
    "\n",
    "\n",
    "\n",
    "        grads = torch.autograd.grad(output_sf[0,cls],model_scrubf.parameters(),retain_graph=False)\n",
    "\n",
    "\n",
    "\n",
    "        G_sf = torch.stack(G_sf)#.pow(2)\n",
    "\n",
    "        delta_f_sf_update = torch.matmul(G_sf,delta_w_s.sqrt()*torch.empty_like(delta_w_s).normal_())\n",
    "\n",
    "        G_sf = G_sf.pow(2)\n",
    "\n",
    "        delta_f_sf = torch.matmul(G_sf,delta_w_s)\n",
    "\n",
    "\n",
    "\n",
    "        output_m0 = modelf0(input)\n",
    "\n",
    "        G_m0 = []\n",
    "\n",
    "\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "\n",
    "            grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=True)\n",
    "\n",
    "            grad_m0 = torch.cat([g.view(-1) for g in grads])\n",
    "\n",
    "            G_m0.append(grad_m0)\n",
    "\n",
    "\n",
    "\n",
    "        grads = torch.autograd.grad(output_m0[0,cls],modelf0.parameters(),retain_graph=False)\n",
    "\n",
    "\n",
    "\n",
    "        G_m0 = torch.stack(G_m0).pow(2)\n",
    "\n",
    "        delta_f_m0 = torch.matmul(G_m0,delta_w_m0)\n",
    "\n",
    "\n",
    "\n",
    "        kl = ((output_m0 - output_sf).pow(2)/delta_f_m0 + delta_f_sf/delta_f_m0 - torch.log(delta_f_sf/delta_f_m0) - 1).sum()\n",
    "\n",
    "\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        output_sf += delta_f_sf_update#delta_f_sf.sqrt()*torch.empty_like(delta_f_sf).normal_()\n",
    "\n",
    "\n",
    "\n",
    "        loss = loss_fn(output_sf, target)\n",
    "\n",
    "        metrics.update(n=input.size(0), loss=loss.item(), error=get_error(output_sf, target), kl=kl.item())\n",
    "\n",
    "\n",
    "\n",
    "    return metrics.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTp8dalsgkmb"
   },
   "outputs": [],
   "source": [
    "def get_variance(model1,model2,alpha,seed=1):\n",
    "\n",
    "\n",
    "\n",
    "    delta_w_s = []\n",
    "\n",
    "    delta_w_m0 = []\n",
    "\n",
    "\n",
    "\n",
    "    for i, (k,p) in enumerate(model1.named_parameters()):\n",
    "\n",
    "        mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "        delta_w_s.append(var.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "    for i, (k,p) in enumerate(model2.named_parameters()):\n",
    "\n",
    "        mu, var = get_mean_var(p, False, alpha=alpha)\n",
    "\n",
    "        delta_w_m0.append(var.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "    return torch.cat(delta_w_s), torch.cat(delta_w_m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhUqMullgkmb",
    "outputId": "63a4604d-b439-4e1e-adb0-86eda939e1dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_list = [1e-8,1e-7,1e-6,1e-5]\n",
    "\n",
    "test_error_list = []\n",
    "\n",
    "information_list = []\n",
    "\n",
    "\n",
    "\n",
    "runs=7\n",
    "\n",
    "for s in range(runs):\n",
    "\n",
    "    test_error_list.append([])\n",
    "\n",
    "    information_list.append([])\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "\n",
    "        delta_w_s, delta_w_m0 = get_variance(model_scrubf, modelf0, alpha)\n",
    "\n",
    "        metrics = test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, test_loader_full,seed=s)\n",
    "\n",
    "        test_error_list[s].append(metrics['error'])\n",
    "\n",
    "        information_list[s].append(metrics['kl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKNO253fgkmb"
   },
   "outputs": [],
   "source": [
    "alpha_list = [1e-8,1e-7,1e-6,1e-5]\n",
    "\n",
    "alpha_list = np.ndarray.flatten(np.array([alpha_list for i in range(runs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbKKo17igkmb"
   },
   "outputs": [],
   "source": [
    "test_error_list = np.ndarray.flatten(np.array(test_error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVDKuylDgkmc"
   },
   "outputs": [],
   "source": [
    "information_list = np.ndarray.flatten(np.array(information_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPwd6RiSgkmc"
   },
   "outputs": [],
   "source": [
    "info_dict_act = {}\n",
    "\n",
    "info_dict_act['alpha'] = alpha_list\n",
    "\n",
    "info_dict_act['error'] = test_error_list*100\n",
    "\n",
    "info_dict_act['info'] = information_list\n",
    "\n",
    "df_act = pd.DataFrame(info_dict_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9XZKis0gkmc"
   },
   "source": [
    "### Information in Activations for Different Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-WLEg_Agkmc"
   },
   "outputs": [],
   "source": [
    "model_scrubf = copy.deepcopy(model_scrub)\n",
    "\n",
    "modelf = copy.deepcopy(model)\n",
    "\n",
    "modelf0 = copy.deepcopy(model0)\n",
    "\n",
    "\n",
    "\n",
    "for p in itertools.chain(modelf.parameters(), modelf0.parameters(), model_scrubf.parameters()):\n",
    "\n",
    "    p.data0 = copy.deepcopy(p.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gynx6zE7gkmc",
    "outputId": "52bb8569-4f9a-4c7b-976d-781e0d1d7246"
   },
   "outputs": [],
   "source": [
    "hessian(retain_loader.dataset, model_scrubf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf)\n",
    "\n",
    "hessian(retain_loader.dataset, modelf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stx7-jz0gkmc"
   },
   "outputs": [],
   "source": [
    "datasets={}\n",
    "\n",
    "datasets['Forget_Set']=forget_loader\n",
    "\n",
    "datasets['Retain_Set']=retain_loader\n",
    "\n",
    "datasets['Test_Set']=test_loader_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xP947rbngkmc",
    "outputId": "1170c605-f27a-4726-c9de-a2bf9062a02b"
   },
   "outputs": [],
   "source": [
    "information_list = []\n",
    "\n",
    "\n",
    "\n",
    "runs=1\n",
    "\n",
    "for s in range(runs):\n",
    "\n",
    "    for k in datasets.keys():\n",
    "\n",
    "        delta_w_s, delta_w_m0 = get_variance(model_scrubf, modelf0, 1e-8)\n",
    "\n",
    "        metrics = test_activations(model_scrubf, modelf0, delta_w_s, delta_w_m0, datasets[k],seed=s)\n",
    "\n",
    "        information_list.append(metrics['kl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7RP5hFtgkmc"
   },
   "outputs": [],
   "source": [
    "labels=['Forget Set' ,'Retain Set', 'Test Set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92GGdAIkgkmd"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_info(ax,df,information_list,title,no_barplot):\n",
    "\n",
    "\n",
    "\n",
    "    if no_barplot:\n",
    "\n",
    "        sns.lineplot(x=\"info\", y=\"error\",data=df,ci='sd',ax=ax)\n",
    "\n",
    "        ax.set(xscale=\"log\")#,yscale='log')\n",
    "\n",
    "        ax.set_xlabel('Remaining Information (NATs)',size=16)\n",
    "\n",
    "        ax.set_ylabel('Test Error (%)',size=16)\n",
    "\n",
    "        ax.set_title(title,size=16)\n",
    "\n",
    "        ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "        ax.tick_params(axis=\"x\", labelsize=16)\n",
    "\n",
    "    else:\n",
    "\n",
    "        y_pos = range(len(information_list))\n",
    "\n",
    "        ax.grid(zorder=0)\n",
    "\n",
    "        ax.xaxis.grid(False)\n",
    "\n",
    "        ax.yaxis.grid(True)\n",
    "\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "        ax.bar(y_pos, information_list, align='center', color=matplotlib.cm.get_cmap('tab10')(0.95), width=0.5,capsize=5)\n",
    "\n",
    "        ax.set_title('Information in Activations',size=18)\n",
    "\n",
    "        ax.set_facecolor('whitesmoke')\n",
    "\n",
    "        ax.tick_params(axis=\"y\", labelsize=18)\n",
    "\n",
    "        ax.set_xticks(y_pos)\n",
    "\n",
    "        ax.set_xticklabels(labels=labels, size=18, rotation=45)\n",
    "\n",
    "        ylabel='NATs'\n",
    "\n",
    "        ax.set_ylabel(ylabel,size=18)\n",
    "\n",
    "        ax.set_ylim(bottom=-0.001)\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_facecolor(np.array([231,231,240])/256)#'whitesmoke')\n",
    "\n",
    "    ax.grid(color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lmTM56Hgkmd",
    "outputId": "fa2d4737-449d-492b-98f8-7cb2ca348494"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(1+3*4.5,4))\n",
    "\n",
    "plot_info(ax[0],df,None,'Information in Weights',True)\n",
    "\n",
    "plot_info(ax[1],df_act,None,'Information in Activations',True)\n",
    "\n",
    "plot_info(ax[2],None,information_list,'Information in Activations',False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('Plots/information.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K50pzTR8gkmd"
   },
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVRpCrOEgkmd",
    "outputId": "3435316b-1b0b-4a5e-8549-0b23325551bb"
   },
   "outputs": [],
   "source": [
    "model_ft = copy.deepcopy(model)\n",
    "\n",
    "retain_loader = replace_loader_dataset(train_loader_full,retain_dataset, seed=seed, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "finetune(model_ft, retain_loader, epochs=10, quiet=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtGUJMdBgkmd",
    "outputId": "80f885d4-5a1a-4475-a2e1-8ebd4fe17a53"
   },
   "outputs": [],
   "source": [
    "finetune_D_r_activations,finetune_D_r_predictions=activations_predictions(model_ft,retain_loader,'Finetune_D_r')\n",
    "\n",
    "finetune_D_f_activations,finetune_D_f_predictions=activations_predictions(model_ft,forget_loader,'Finetune_D_f')\n",
    "\n",
    "finetune_D_t_activations,finetune_D_t_predictions=activations_predictions(model_ft,test_loader_full,'Finetune_D_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yW4F9uBJgkmd",
    "outputId": "d1d8575a-87da-49e8-9ed7-33498ea77a6a"
   },
   "outputs": [],
   "source": [
    "predictions_distance(m0_D_f_predictions,finetune_D_f_predictions,'Retrain_Finetune_D_f')\n",
    "\n",
    "activations_distance(m0_D_f_activations,finetune_D_f_activations,'Retrain_Finetune_D_f')\n",
    "\n",
    "activations_distance(m0_D_r_activations,finetune_D_r_activations,'Retrain_Finetune_D_r')\n",
    "\n",
    "activations_distance(m0_D_t_activations,finetune_D_t_activations,'Retrain_Finetune_D_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV_Nftbtgkmd"
   },
   "source": [
    "### Readouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0NWWpFJgkme",
    "outputId": "325fff21-65e4-4a5e-9506-d424f998d2bb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: readouts\n",
    "\n",
    "except: readouts = {}\n",
    "\n",
    "\n",
    "\n",
    "thresh=log_dict['Original_Model_D_f_loss']+1e-5\n",
    "\n",
    "readouts[\"e\"] = all_readouts(copy.deepcopy(model),thresh,'Original')\n",
    "\n",
    "readouts[\"a\"] = all_readouts(copy.deepcopy(model_ft),thresh,'Finetune')\n",
    "\n",
    "readouts[\"b\"] = all_readouts(copy.deepcopy(modelf),thresh,'Fisher')\n",
    "\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_scrub),thresh,'NTK')\n",
    "\n",
    "readouts[\"c\"] = all_readouts(copy.deepcopy(model_scrubf),thresh,'NTK_Fisher')\n",
    "\n",
    "readouts[\"d\"] = all_readouts(copy.deepcopy(model0),thresh,'Retrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3EEsYOrgkme"
   },
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lspQUzBVgkme"
   },
   "outputs": [],
   "source": [
    "np.save(f\"logs/{m0_name.split('/')[1].split('.')[0]}.npy\",log_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WxhPTg9gkme"
   },
   "source": [
    "### Activations along the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBrUH-EHgkme"
   },
   "source": [
    "##### Membership Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1floZBfgkme"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "def entropy(p, dim = -1, keepdim = False):\n",
    "\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim=dim, keepdim=keepdim)\n",
    "\n",
    "\n",
    "\n",
    "def collect_prob(data_loader, model):\n",
    "\n",
    "\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(data_loader.dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(data_loader, leave=False)):\n",
    "\n",
    "            batch = [tensor.to(next(model.parameters()).device) for tensor in batch]\n",
    "\n",
    "            data, target = batch\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            prob.append(F.softmax(output, dim=-1).data)\n",
    "\n",
    "    return torch.cat(prob)\n",
    "\n",
    "\n",
    "\n",
    "def get_membership_attack_data(retain_loader, forget_loader, test_loader, model):\n",
    "\n",
    "    retain_prob = collect_prob(retain_loader, model)\n",
    "\n",
    "    forget_prob = collect_prob(forget_loader, model)\n",
    "\n",
    "    test_prob = collect_prob(test_loader, model)\n",
    "\n",
    "\n",
    "\n",
    "    X_r = torch.cat([entropy(retain_prob), entropy(test_prob)]).cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    Y_r = np.concatenate([np.ones(len(retain_prob)), np.zeros(len(test_prob))])\n",
    "\n",
    "\n",
    "\n",
    "    X_f = entropy(forget_prob).cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    Y_f = np.concatenate([np.ones(len(forget_prob))])\n",
    "\n",
    "    return X_f, Y_f, X_r, Y_r\n",
    "\n",
    "\n",
    "\n",
    "def get_membership_attack_prob(retain_loader, forget_loader, test_loader, model):\n",
    "\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(retain_loader, forget_loader, test_loader, model)\n",
    "\n",
    "    clf = SVC(C=3,gamma='auto',kernel='rbf')\n",
    "\n",
    "    #clf = LogisticRegression(class_weight='balanced',solver='lbfgs',multi_class='multinomial')\n",
    "\n",
    "    clf.fit(X_r, Y_r)\n",
    "\n",
    "    results = clf.predict(X_f)\n",
    "\n",
    "    return results.mean()\n",
    "\n",
    "\n",
    "\n",
    "def plot_entropy_dist(model, ax, title):\n",
    "\n",
    "    train_loader_full, test_loader_full = datasets.get_loaders(dataset, batch_size=100, seed=0, augment=False, shuffle=False)\n",
    "\n",
    "    indexes = np.flatnonzero(np.array(train_loader_full.dataset.targets) == class_to_forget)\n",
    "\n",
    "    replaced = np.random.RandomState(0).choice(indexes, size=100 if num_to_forget==100 else len(indexes), replace=False)\n",
    "\n",
    "    X_f, Y_f, X_r, Y_r = get_membership_attack_data(train_loader_full, test_loader_full, model, replaced)\n",
    "\n",
    "    sns.distplot(np.log(X_r[Y_r==1]).reshape(-1), kde=False, norm_hist=True, rug=False, label='retain', ax=ax)\n",
    "\n",
    "    sns.distplot(np.log(X_r[Y_r==0]).reshape(-1), kde=False, norm_hist=True, rug=False, label='test', ax=ax)\n",
    "\n",
    "    sns.distplot(np.log(X_f).reshape(-1), kde=False, norm_hist=True, rug=False, label='forget', ax=ax)\n",
    "\n",
    "    ax.legend(prop={'size': 14})\n",
    "\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    ax.set_title(title,size=18)\n",
    "\n",
    "    ax.set_xlabel('Log of Entropy',size=14)\n",
    "\n",
    "    ax.set_ylim(0,0.4)\n",
    "\n",
    "    ax.set_xlim(-35,2)\n",
    "\n",
    "\n",
    "\n",
    "def membership_attack(retain_loader,forget_loader,test_loader,model):\n",
    "\n",
    "    prob = get_membership_attack_prob(retain_loader,forget_loader,test_loader,model)\n",
    "\n",
    "    print(\"Attack prob: \", prob)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TZagNsWgkmf"
   },
   "outputs": [],
   "source": [
    "attack_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_grhly8Vgkmf",
    "outputId": "b8140456-4b02-4450-af19-5fd88f224b37"
   },
   "outputs": [],
   "source": [
    "attack_dict['Original']=membership_attack(retain_loader,forget_loader,test_loader_full,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKywkiRSgkmf",
    "outputId": "c4bfebbf-9318-4807-ea8e-49caf565c3bd"
   },
   "outputs": [],
   "source": [
    "attack_dict['Retrain']=membership_attack(retain_loader,forget_loader,test_loader_full,model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyX-taVTgkmf",
    "outputId": "1f26358e-d13d-4131-9e61-246339f42057"
   },
   "outputs": [],
   "source": [
    "attack_dict['NTK']=membership_attack(retain_loader,forget_loader,test_loader_full,model_scrub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd7GYBfUgkmf",
    "outputId": "caeae911-9777-4571-9af5-a366b5c77b74"
   },
   "outputs": [],
   "source": [
    "attack_dict['Fisher']=membership_attack(retain_loader,forget_loader,test_loader_full,modelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8ECQ3CNgkmf",
    "outputId": "7b603c5f-607c-4fd4-92c9-d6392fae6efd"
   },
   "outputs": [],
   "source": [
    "attack_dict['Finetune']=membership_attack(retain_loader,forget_loader,test_loader_full,model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0tl1kaVgkmf",
    "outputId": "c036cf14-c533-4510-e3a2-dfa1aad39280"
   },
   "outputs": [],
   "source": [
    "attack_dict['Fisher_NTK']=membership_attack(retain_loader,forget_loader,test_loader_full,model_scrubf)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
